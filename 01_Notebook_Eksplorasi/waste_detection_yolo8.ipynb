{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waste Detection with YOLOv8 and FastAPI\n",
    "\n",
    "This notebook sets up and runs a waste detection system using YOLOv8 for training and inference, and FastAPI for serving the model. It includes steps to download the dataset from Roboflow, train the model, run the FastAPI app with `best.pt`, and optionally convert the model to RKNN format for deployment on OrangePi5Pro.\n",
    "\n",
    "**Dataset:**\n",
    "- The dataset is sourced from Roboflow Universe: [Waste Detection Dataset](https://universe.roboflow.com/ai-project-i3wje/waste-detection-vqkjo).\n",
    "- It contains 9178 images of recyclable and non-recyclable waste, annotated for object detection with 22 classes.\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3.8-3.11 with dependencies listed in `requirements.txt`.\n",
    "- A Roboflow API key to download the dataset (sign up at [Roboflow](https://roboflow.com) to get one).\n",
    "- GPU (optional but recommended for training).\n",
    "- Webcam (for testing FastAPI video feed).\n",
    "\n",
    "**Steps:**\n",
    "1. Download and set up the dataset from Roboflow.\n",
    "2. Train the YOLOv8 model.\n",
    "3. Run the FastAPI app with `best.pt`.\n",
    "4. (Optional) Convert `best.pt` to RKNN and run FastAPI with RKNN model.\n",
    "\n",
    "**Dataset Setup Tutorial:**\n",
    "1. **Obtain Roboflow API Key:**\n",
    "   - Sign up or log in at [Roboflow](https://roboflow.com).\n",
    "   - Navigate to your account settings to find your API key.\n",
    "   - Provide this key when prompted in the dataset download cell below.\n",
    "\n",
    "2. **Download the Dataset:**\n",
    "   - The notebook will download the dataset using the Roboflow API to the `dataset` directory.\n",
    "   - The dataset will be in YOLO format, with `train`, `valid`, and `test` folders, each containing `images` and `labels` subfolders.\n",
    "   - Alternatively, you can manually download the dataset from [https://universe.roboflow.com/ai-project-i3wje/waste-detection-vqkjo](https://universe.roboflow.com/ai-project-i3wje/waste-detection-vqkjo) in YOLOv8 format and extract it to the `dataset` directory.\n",
    "\n",
    "3. **Place the Dataset Folders:**\n",
    "   - After downloading, the dataset will be automatically placed in the `dataset` directory with the following structure:\n",
    "     ```\n",
    "     dataset/\n",
    "     ├── train/\n",
    "     │   ├── images/\n",
    "     │   │   ├── image1.jpg\n",
    "     │   │   ├── image2.jpg\n",
    "     │   │   └── ...\n",
    "     │   └── labels/\n",
    "     │       ├── image1.txt\n",
    "     │       ├── image2.txt\n",
    "     │       └── ...\n",
    "     ├── valid/\n",
    "     │   ├── images/\n",
    "     │   └── labels/\n",
    "     ├── test/\n",
    "     │   ├── images/\n",
    "     │   └── labels/\n",
    "     └── data.yaml\n",
    "     ```\n",
    "   - Ensure the `data.yaml` file is in the `dataset` directory, as it specifies the paths to `train`, `valid`, and `test` images and the 22 class names.\n",
    "   - Do not modify the folder structure or `data.yaml` unless you know what you're doing, as the training script relies on this setup.\n",
    "\n",
    "4. **Verify the Dataset:**\n",
    "   - The notebook includes a verification step to check that the `train`, `valid`, and `test` directories contain images.\n",
    "   - If you encounter errors, ensure the dataset was downloaded correctly and the folders are populated.\n",
    "\n",
    "Run all cells sequentially to execute the pipeline. If you already have a trained `best.pt`, place it in the `weights` directory and skip the training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating project directories under: d:\\ipynb\n",
      "Project directories created: dataset, weights, static\n",
      "Changing current directory to: d:\\ipynb\\dataset\n",
      "d:\\ipynb\\dataset\n",
      "Starting dataset download from Roboflow...\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in waste-detection-9 to yolov8:: 100%|██████████| 650481/650481 [02:17<00:00, 4724.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to waste-detection-9 in yolov8:: 100%|██████████| 15264/15264 [00:09<00:00, 1582.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roboflow dataset downloaded to: d:\\ipynb\\dataset\\waste-detection-9\n",
      "\n",
      "data.yaml successfully created/updated at: d:\\ipynb\\dataset\\waste-detection-9\\data.yaml\n",
      "\n",
      "Dataset paths defined:\n",
      "DATASET_DIR: d:\\ipynb\\dataset\\waste-detection-9\n",
      "TRAIN_IMAGES: d:\\ipynb\\dataset\\waste-detection-9\\train\\images\n",
      "VAL_IMAGES: d:\\ipynb\\dataset\\waste-detection-9\\valid\\images\n",
      "TEST_IMAGES: d:\\ipynb\\dataset\\waste-detection-9\\test\\images\n",
      "\n",
      "Verifying data.yaml content:\n",
      "train: waste-detection-9/train/images\n",
      "val: waste-detection-9/valid/images\n",
      "test: waste-detection-9/test/images\n",
      "\n",
      "nc: 22\n",
      "names: ['battery', 'can', 'cardboard_bowl', 'cardboard_box', 'chemical_plastic_bottle', 'chemical_plastic_gallon', 'chemical_spray_can', 'light_bulb', 'paint_bucket', 'plastic_bag', 'plastic_bottle', 'plastic_bottle_cap', 'plastic_box', 'plastic_cultery', 'plastic_cup', 'plastic_cup_lid', 'reuseable_paper', 'scrap_paper', 'scrap_plastic', 'snack_bag', 'stick', 'straw']\n",
      "data.yaml content verified.\n",
      "\n",
      "Dataset downloaded and directory structure verified.\n",
      "Ready to proceed with training or further steps!\n"
     ]
    }
   ],
   "source": [
    "# Install Roboflow library if not already installed\n",
    "# --quiet flag suppresses verbose output for cleaner notebook\n",
    "!pip install ultralytics --quiet\n",
    "!pip install torch --quiet\n",
    "!pip install onnxruntime --quiet\n",
    "!pip install roboflow --quiet\n",
    "\n",
    "# Import necessary libraries for file handling\n",
    "import os\n",
    "from pathlib import Path\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# Define base directory: current working directory of the notebook\n",
    "BASE_DIR = Path.cwd()\n",
    "\n",
    "# Create necessary project directories\n",
    "print(f\"Creating project directories under: {BASE_DIR}\")\n",
    "os.makedirs(BASE_DIR / 'dataset', exist_ok=True)\n",
    "os.makedirs(BASE_DIR / 'weights', exist_ok=True)\n",
    "os.makedirs(BASE_DIR / 'static', exist_ok=True)\n",
    "print(\"Project directories created: dataset, weights, static\")\n",
    "\n",
    "# Change directory to where the dataset will be downloaded.\n",
    "# Roboflow will create a subfolder (e.g., 'waste-detection-9') inside this.\n",
    "print(f\"Changing current directory to: {BASE_DIR / 'dataset'}\")\n",
    "%cd {BASE_DIR}/dataset\n",
    "\n",
    "# Download dataset from Roboflow\n",
    "# NOTE: Roboflow automatically generates a data.yaml file\n",
    "# inside the downloaded dataset folder. You typically use that.\n",
    "print(\"Starting dataset download from Roboflow...\")\n",
    "rf = Roboflow(api_key=\"HI42QRXkU9xSlq7DHhks\")\n",
    "project = rf.workspace(\"ai-project-i3wje\").project(\"waste-detection-vqkjo\")\n",
    "dataset = project.version(9).download(\"yolov8\") # This creates a 'waste-detection-9' folder (or similar)\n",
    "\n",
    "# Get the actual path where Roboflow downloaded the dataset.\n",
    "# The `dataset.location` attribute gives the path to the main dataset folder (e.g., 'BASE_DIR/dataset/waste-detection-9')\n",
    "ROBOFLOW_DATASET_ROOT = Path(dataset.location)\n",
    "print(f\"Roboflow dataset downloaded to: {ROBOFLOW_DATASET_ROOT}\")\n",
    "\n",
    "# --- Creating/Verifying data.yaml consistency ---\n",
    "# Define the content of your data.yaml file.\n",
    "# These paths are relative to the ROBOFLOW_DATASET_ROOT (e.g., 'waste-detection-9/')\n",
    "# This ensures consistency with the Roboflow's internal structure.\n",
    "data_yaml_content = f\"\"\"\n",
    "train: {ROBOFLOW_DATASET_ROOT.name}/train/images\n",
    "val: {ROBOFLOW_DATASET_ROOT.name}/valid/images\n",
    "test: {ROBOFLOW_DATASET_ROOT.name}/test/images\n",
    "\n",
    "nc: 22\n",
    "names: ['battery', 'can', 'cardboard_bowl', 'cardboard_box', 'chemical_plastic_bottle', 'chemical_plastic_gallon', 'chemical_spray_can', 'light_bulb', 'paint_bucket', 'plastic_bag', 'plastic_bottle', 'plastic_bottle_cap', 'plastic_box', 'plastic_cultery', 'plastic_cup', 'plastic_cup_lid', 'reuseable_paper', 'scrap_paper', 'scrap_plastic', 'snack_bag', 'stick', 'straw']\n",
    "\"\"\"\n",
    "\n",
    "# Define the full path for the data.yaml file.\n",
    "# It's usually placed directly inside the main dataset folder created by Roboflow.\n",
    "data_yaml_path = ROBOFLOW_DATASET_ROOT / 'data.yaml'\n",
    "\n",
    "# Write the content to the data.yaml file, overwriting if it exists\n",
    "# (Roboflow already creates one, but we ensure its content matches if needed)\n",
    "try:\n",
    "    with open(data_yaml_path, 'w') as f:\n",
    "        f.write(data_yaml_content.strip())\n",
    "    print(f\"\\ndata.yaml successfully created/updated at: {data_yaml_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing data.yaml: {e}\")\n",
    "\n",
    "# Define dataset paths based on the actual Roboflow download structure\n",
    "# Note: DATASET_DIR here is the BASE_DIR / 'dataset'\n",
    "# The actual data is inside ROBOFLOW_DATASET_ROOT (e.g., BASE_DIR / 'dataset' / 'waste-detection-9')\n",
    "DATASET_DIR = ROBOFLOW_DATASET_ROOT # This now points to the correct root for the dataset\n",
    "TRAIN_IMAGES = DATASET_DIR / 'train' / 'images'\n",
    "VAL_IMAGES = DATASET_DIR / 'valid' / 'images'\n",
    "TEST_IMAGES = DATASET_DIR / 'test' / 'images'\n",
    "\n",
    "print('\\nDataset paths defined:')\n",
    "print(f'DATASET_DIR: {DATASET_DIR}')\n",
    "print(f'TRAIN_IMAGES: {TRAIN_IMAGES}')\n",
    "print(f'VAL_IMAGES: {VAL_IMAGES}')\n",
    "print(f'TEST_IMAGES: {TEST_IMAGES}')\n",
    "# --- Verification of data.yaml existence and content ---\n",
    "if not data_yaml_path.exists():\n",
    "    raise FileNotFoundError(f'data.yaml not found at {data_yaml_path}. Ensure dataset downloaded correctly or path is wrong.')\n",
    "\n",
    "# Read and verify data.yaml content\n",
    "print('\\nVerifying data.yaml content:')\n",
    "try:\n",
    "    with open(data_yaml_path, 'r') as f:\n",
    "        data_yaml_content_read = f.read()\n",
    "    print(data_yaml_content_read)\n",
    "    print(\"data.yaml content verified.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading data.yaml for verification: {e}\")\n",
    "\n",
    "print('\\nDataset downloaded and directory structure verified.')\n",
    "print('Ready to proceed with training or further steps!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.py created successfully at: d:\\ipynb\\train.py\n",
      "Changing current directory back to: d:\\ipynb\n",
      "d:\\ipynb\n",
      "Running train.py...\n",
      "Starting training...\n",
      "New https://pypi.org/project/ultralytics/8.3.156 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.151  Python-3.12.6 torch-2.7.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=d:\\ipynb\\dataset\\waste-detection-9\\data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=waste_detection4, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\waste_detection4, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=22\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    755602  ultralytics.nn.modules.head.Detect           [22, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,015,138 parameters, 3,015,122 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 804.1485.0 MB/s, size: 87.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\ipynb\\dataset\\waste-detection-9\\train\\labels.cache... 6684 images, 12 backgrounds, 0 corrupt: 100%|██████████| 6684/6684 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Box and segment counts should be equal, but got len(segments) = 3078, len(boxes) = 14878. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 4.22.4 MB/s, size: 39.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\ipynb\\dataset\\waste-detection-9\\valid\\labels... 628 images, 0 backgrounds, 0 corrupt: 100%|██████████| 628/628 [00:01<00:00, 370.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\ipynb\\dataset\\waste-detection-9\\valid\\labels.cache\n",
      "WARNING Box and segment counts should be equal, but got len(segments) = 278, len(boxes) = 1273. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\waste_detection4\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000385, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\waste_detection4\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50      2.15G      1.444      3.561      1.627         65        640: 100%|██████████| 418/418 [01:33<00:00,  4.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 20/20 [00:04<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        628       1273      0.621      0.391      0.457        0.3\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50      2.09G      1.364      2.704      1.585         56        640:   6%|▌         | 24/418 [00:05<01:24,  4.65it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ipynb\\train.py:57\u001b[39m\n\u001b[32m     55\u001b[39m         name = \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     56\u001b[39m     sys.modules[\u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m].__spec__ = DummySpec()\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ipynb\\train.py:18\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m train_results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_yaml\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m640\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwaste_detection\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     25\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mStarting validation...\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\model.py:797\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    794\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m    796\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.hub_session = \u001b[38;5;28mself\u001b[39m.session  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m797\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\trainer.py:227\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    224\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\trainer.py:419\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self, world_size)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ni - last_opt_step >= \u001b[38;5;28mself\u001b[39m.accumulate:\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    420\u001b[39m     last_opt_step = ni\n\u001b[32m    422\u001b[39m     \u001b[38;5;66;03m# Timed stopping\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ultralytics\\engine\\trainer.py:638\u001b[39m, in \u001b[36mBaseTrainer.optimizer_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    636\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.unscale_(\u001b[38;5;28mself\u001b[39m.optimizer)  \u001b[38;5;66;03m# unscale gradients\u001b[39;00m\n\u001b[32m    637\u001b[39m torch.nn.utils.clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.model.parameters(), max_norm=\u001b[32m10.0\u001b[39m)  \u001b[38;5;66;03m# clip gradients\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.update()\n\u001b[32m    640\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\amp\\grad_scaler.py:461\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    455\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    458\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    459\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    463\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\amp\\grad_scaler.py:355\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    349\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m     **kwargs: Any,\n\u001b[32m    353\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    354\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf_per_device\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    356\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\amp\\grad_scaler.py:355\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    349\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m     **kwargs: Any,\n\u001b[32m    353\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    354\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    356\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'runs/train/waste_detection' directory not found. Training might have failed or been interrupted.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Training did not produce best.pt. Check training logs for errors.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 100\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     99\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mError: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbest.pt\u001b[39m\u001b[33m'\u001b[39m\u001b[33m not found in \u001b[39m\u001b[33m'\u001b[39m\u001b[33mruns/train/waste_detection/weights/\u001b[39m\u001b[33m'\u001b[39m\u001b[33m. Check training logs for details.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mTraining did not produce best.pt. Check training logs for errors.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Training did not produce best.pt. Check training logs for errors."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries (ensure they are imported if this cell is run independently)\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Assume BASE_DIR and ROBOFLOW_DATASET_ROOT are defined from previous cells\n",
    "# If running this cell standalone, uncomment and define them:\n",
    "# BASE_DIR = Path.cwd()\n",
    "# ROBOFLOW_DATASET_ROOT = BASE_DIR / 'dataset' / 'waste-detection-9' # Adjust 'waste-detection-9' to your actual folder name\n",
    "\n",
    "# --- Create train.py and run it ---\n",
    "# train.py content\n",
    "train_py_content = f\"\"\"from ultralytics import YOLO\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import sys # Added this import to handle __spec__ issue\n",
    "\n",
    "# Absolute path to the Roboflow dataset root, passed from the notebook\n",
    "ROBOFLOW_DATASET_ROOT_ABS_PATH = r\"{ROBOFLOW_DATASET_ROOT.as_posix()}\"\n",
    "\n",
    "def main():\n",
    "    # Define path to data.yaml using the absolute path\n",
    "    data_yaml = str(Path(ROBOFLOW_DATASET_ROOT_ABS_PATH) / 'data.yaml')\n",
    "\n",
    "    # Load YOLOv8n model (pre-trained)\n",
    "    model = YOLO('yolov8n.pt')\n",
    "\n",
    "    # Training\n",
    "    print('Starting training...')\n",
    "    train_results = model.train(\n",
    "        data=data_yaml,\n",
    "        epochs=50,\n",
    "        imgsz=640,\n",
    "        batch=16,\n",
    "        device=0 if torch.cuda.is_available() else 'cpu',\n",
    "        name='waste_detection'\n",
    "    )\n",
    "\n",
    "    # Validation\n",
    "    print('Starting validation...')\n",
    "    val_results = model.val(\n",
    "        data=data_yaml,\n",
    "        imgsz=640,\n",
    "        batch=16,\n",
    "        device=0 if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "\n",
    "    # Prediction on test data\n",
    "    print('Starting prediction...')\n",
    "    predict_results = model.predict(\n",
    "        source=str(Path(ROBOFLOW_DATASET_ROOT_ABS_PATH) / 'test' / 'images'),\n",
    "        save=True,\n",
    "        imgsz=640,\n",
    "        device=0 if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "\n",
    "    print('Process completed!')\n",
    "    print(f'Training results saved at: runs/train/waste_detection')\n",
    "    print(f'Best model: runs/train/waste_detection/weights/best.pt')\n",
    "    print(f'Prediction results saved at: runs/predict/')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Workaround for AttributeError: module '__main__' has no attribute '__spec__'\n",
    "    # when running with %run in some Jupyter/IPython environments.\n",
    "    if not hasattr(sys.modules['__main__'], '__spec__'):\n",
    "        class DummySpec:\n",
    "            name = '__main__'\n",
    "        sys.modules['__main__'].__spec__ = DummySpec()\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "# Write train.py to the BASE_DIR\n",
    "with open(BASE_DIR / 'train.py', 'w') as f:\n",
    "    f.write(train_py_content)\n",
    "\n",
    "print('train.py created successfully at:', BASE_DIR / 'train.py')\n",
    "\n",
    "# Change directory back to BASE_DIR before running train.py\n",
    "print(f\"Changing current directory back to: {BASE_DIR}\")\n",
    "%cd {BASE_DIR}\n",
    "\n",
    "# Run training\n",
    "# Note: This step may take significant time depending on your hardware and dataset size.\n",
    "# If you already have a trained 'best.pt', you can skip this cell and copy 'best.pt' to the 'weights' directory.\n",
    "print(\"Running train.py...\")\n",
    "%run train.py\n",
    "\n",
    "# Move best.pt to weights directory\n",
    "best_pt_source = BASE_DIR / 'runs' / 'train' / 'waste_detection' / 'weights' / 'best.pt'\n",
    "best_pt_dest = BASE_DIR / 'weights' / 'best.pt'\n",
    "\n",
    "if best_pt_source.exists():\n",
    "    shutil.move(best_pt_source, best_pt_dest)\n",
    "    print('best.pt moved to weights directory.')\n",
    "else:\n",
    "    # Check common reasons for failure\n",
    "    if not (BASE_DIR / 'runs' / 'train' / 'waste_detection').exists():\n",
    "        print(\"Error: 'runs/train/waste_detection' directory not found. Training might have failed or been interrupted.\")\n",
    "    else:\n",
    "        print(\"Error: 'best.pt' not found in 'runs/train/waste_detection/weights/'. Check training logs for details.\")\n",
    "    raise FileNotFoundError('Training did not produce best.pt. Check training logs for errors.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "settings.py created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create settings.py\n",
    "settings_py_content = \"\"\"from pathlib import Path\n",
    "import sys\n",
    "\n",
    "file_path = Path(__file__).resolve()\n",
    "root_path = file_path.parent\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(str(root_path))\n",
    "ROOT = root_path.relative_to(Path.cwd())\n",
    "\n",
    "# ML Model config\n",
    "MODEL_DIR = ROOT / 'weights'\n",
    "DETECTION_MODEL = MODEL_DIR / 'best.pt'\n",
    "\n",
    "MODEL_INPUT_WIDTH = 640\n",
    "MODEL_INPUT_HEIGHT = 640\n",
    "CONF_THRESHOLD = 0.25\n",
    "NMS_IOU_THRESHOLD = 0.45\n",
    "\n",
    "# Class names\n",
    "ALL_CLASSES = [\n",
    "    'battery', 'can', 'cardboard_bowl', 'cardboard_box', 'chemical_plastic_bottle',\n",
    "    'chemical_plastic_gallon', 'chemical_spray_can', 'light_bulb', 'paint_bucket',\n",
    "    'plastic_bag', 'plastic_bottle', 'plastic_bottle_cap', 'plastic_box',\n",
    "    'plastic_cultery', 'plastic_cup', 'plastic_cup_lid', 'reuseable_paper',\n",
    "    'scrap_paper', 'scrap_plastic', 'snack_bag', 'stick', 'straw'\n",
    "]\n",
    "\n",
    "# Webcam\n",
    "WEBCAM_PATH = 0\n",
    "\n",
    "# Waste type classification\n",
    "RECYCLABLE = ['cardboard_box', 'can', 'plastic_bottle_cap', 'plastic_bottle', 'reuseable_paper']\n",
    "NON_RECYCLABLE = [\n",
    "    'plastic_bag', 'scrap_paper', 'stick', 'plastic_cup', 'snack_bag',\n",
    "    'plastic_box', 'straw', 'plastic_cup_lid', 'scrap_plastic',\n",
    "    'cardboard_bowl', 'plastic_cultery'\n",
    "]\n",
    "HAZARDOUS = [\n",
    "    'battery', 'chemical_spray_can', 'chemical_plastic_bottle',\n",
    "    'chemical_plastic_gallon', 'light_bulb', 'paint_bucket'\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "with open(BASE_DIR / 'settings.py', 'w') as f:\n",
    "    f.write(settings_py_content)\n",
    "\n",
    "print('settings.py created successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script.js created successfully at: d:\\ipynb\\static\\script.js\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pathlib import Path\n",
    "\n",
    "# Assume BASE_DIR is already defined from previous cells or define it here if running standalone\n",
    "BASE_DIR = Path.cwd() # Adjust if your BASE_DIR is different\n",
    "\n",
    "# script.js content\n",
    "script_js_content = \"\"\"document.addEventListener('DOMContentLoaded', () => {\n",
    "    const dropArea = document.getElementById('drop-area');\n",
    "    const fileInput = document.getElementById('file-input');\n",
    "    const uploadBtn = document.getElementById('upload-btn');\n",
    "    const imagePreview = document.getElementById('image-preview');\n",
    "    const previewImage = imagePreview.querySelector('img');\n",
    "    const uploadStatus = document.getElementById('upload-status');\n",
    "    const resultImage = document.getElementById('result-image');\n",
    "    const resultText = document.getElementById('result-text');\n",
    "\n",
    "    const startWebcamBtn = document.getElementById('start-webcam');\n",
    "    const stopWebcamBtn = document.getElementById('stop-webcam');\n",
    "    const webcam = document.getElementById('webcam');\n",
    "    const webcamResult = document.getElementById('webcam-result');\n",
    "    const webcamStatus = document.getElementById('webcam-status');\n",
    "\n",
    "    let pollClassificationInterval = null;\n",
    "\n",
    "    const showMessage = (element, message, type = 'info') => {\n",
    "        element.classList.remove('hidden', 'text-green-500', 'text-red-500', 'text-blue-500', 'bg-green-100', 'bg-red-100', 'bg-blue-100');\n",
    "        element.classList.add('p-2', 'rounded');\n",
    "        if (type === 'success') {\n",
    "            element.classList.add('bg-green-100', 'text-green-500');\n",
    "        } else if (type === 'error') {\n",
    "            element.classList.add('bg-red-100', 'text-red-500');\n",
    "        } else if (type === 'info') {\n",
    "            element.classList.add('bg-blue-100', 'text-blue-500');\n",
    "        }\n",
    "        element.innerHTML = message;\n",
    "    };\n",
    "\n",
    "    const hideMessage = (element) => {\n",
    "        element.classList.add('hidden');\n",
    "        element.innerHTML = '';\n",
    "    };\n",
    "\n",
    "    // --- Image Upload Functionality ---\n",
    "    dropArea.addEventListener('dragover', (e) => {\n",
    "        e.preventDefault();\n",
    "        dropArea.classList.add('border-blue-500', 'bg-blue-50');\n",
    "    });\n",
    "\n",
    "    dropArea.addEventListener('dragleave', () => {\n",
    "        dropArea.classList.remove('border-blue-500', 'bg-blue-50');\n",
    "    });\n",
    "\n",
    "    dropArea.addEventListener('drop', (e) => {\n",
    "        e.preventDefault();\n",
    "        dropArea.classList.remove('border-blue-500', 'bg-blue-50');\n",
    "        const file = e.dataTransfer.files[0];\n",
    "        if (file && file.type.startsWith('image/')) {\n",
    "            fileInput.files = e.dataTransfer.files;\n",
    "            displayImagePreview(file);\n",
    "        } else {\n",
    "            showMessage(uploadStatus, 'Please drop an image file.', 'error');\n",
    "        }\n",
    "    });\n",
    "\n",
    "    dropArea.addEventListener('click', () => {\n",
    "        fileInput.click();\n",
    "    });\n",
    "\n",
    "    fileInput.addEventListener('change', () => {\n",
    "        if (fileInput.files.length > 0) {\n",
    "            displayImagePreview(fileInput.files[0]);\n",
    "        } else {\n",
    "            imagePreview.classList.add('hidden');\n",
    "            previewImage.src = '#';\n",
    "        }\n",
    "    });\n",
    "\n",
    "    function displayImagePreview(file) {\n",
    "        const reader = new FileReader();\n",
    "        reader.onload = (e) => {\n",
    "            previewImage.src = e.target.result;\n",
    "            imagePreview.classList.remove('hidden');\n",
    "        };\n",
    "        reader.readAsDataURL(file);\n",
    "    }\n",
    "\n",
    "    uploadBtn.addEventListener('click', async () => {\n",
    "        if (fileInput.files.length === 0) {\n",
    "            showMessage(uploadStatus, 'Please select an image to upload!', 'error');\n",
    "            return;\n",
    "        }\n",
    "\n",
    "        hideMessage(uploadStatus);\n",
    "        resultImage.innerHTML = '';\n",
    "        resultText.innerHTML = '';\n",
    "        showMessage(uploadStatus, 'Uploading and detecting...', 'info');\n",
    "\n",
    "        const formData = new FormData();\n",
    "        formData.append('file', fileInput.files[0]);\n",
    "\n",
    "        try {\n",
    "            const response = await fetch('/detect', {\n",
    "                method: 'POST',\n",
    "                body: formData\n",
    "            });\n",
    "\n",
    "            if (!response.ok) {\n",
    "                const errorData = await response.json();\n",
    "                throw new Error(errorData.detail || 'Something went wrong during detection.');\n",
    "            }\n",
    "\n",
    "            const data = await response.json();\n",
    "            showMessage(uploadStatus, 'Upload and detection successful!', 'success');\n",
    "\n",
    "            resultImage.innerHTML = `<img src=\"data:image/jpeg;base64,${data.image}\" alt=\"Detected Image\" class=\"w-full rounded\">`;\n",
    "\n",
    "            let resultHtml = '';\n",
    "            if (data.results.recyclable.length > 0) {\n",
    "                resultHtml += `<div class=\"bg-yellow-200 p-4 rounded\"><strong>Recyclable items:</strong><br>- ${data.results.recyclable.join('<br>- ')}</div>`;\n",
    "            }\n",
    "            if (data.results.non_recyclable.length > 0) {\n",
    "                resultHtml += `<div class=\"bg-blue-200 p-4 rounded\"><strong>Non-Recyclable items:</strong><br>- ${data.results.non_recyclable.join('<br>- ')}</div>`;\n",
    "            }\n",
    "            if (data.results.hazardous.length > 0) {\n",
    "                resultHtml += `<div class=\"bg-red-200 p-4 rounded\"><strong>Hazardous items:</strong><br>- ${data.results.hazardous.join('<br>- ')}</div>`;\n",
    "            }\n",
    "            resultText.innerHTML = resultHtml;\n",
    "\n",
    "        } catch (error) {\n",
    "            showMessage(uploadStatus, `Error: ${error.message}`, 'error');\n",
    "            resultImage.innerHTML = '';\n",
    "            resultText.innerHTML = '';\n",
    "        }\n",
    "    });\n",
    "\n",
    "    // --- Webcam Functionality ---\n",
    "    startWebcamBtn.addEventListener('click', async () => {\n",
    "        // Clear previous states\n",
    "        hideMessage(webcamStatus);\n",
    "        webcamResult.innerHTML = '';\n",
    "\n",
    "        showMessage(webcamStatus, 'Starting webcam...', 'info');\n",
    "\n",
    "        // Reset webcam src to force reload the stream\n",
    "        webcam.src = ''; \n",
    "        webcam.src = '/video_feed';\n",
    "        webcam.classList.remove('hidden');\n",
    "        startWebcamBtn.classList.add('hidden');\n",
    "        stopWebcamBtn.classList.remove('hidden');\n",
    "\n",
    "        // Set up onload and onerror for webcam image\n",
    "        webcam.onload = () => {\n",
    "            showMessage(webcamStatus, 'Webcam stream started.', 'success');\n",
    "        };\n",
    "        webcam.onerror = () => {\n",
    "            showMessage(webcamStatus, 'Error loading webcam stream. Check backend logs or webcam settings.', 'error');\n",
    "            webcam.classList.add('hidden');\n",
    "            startWebcamBtn.classList.remove('hidden');\n",
    "            stopWebcamBtn.classList.add('hidden');\n",
    "        };\n",
    "\n",
    "        // Start polling for classification data\n",
    "        if (pollClassificationInterval) {\n",
    "            clearInterval(pollClassificationInterval);\n",
    "        }\n",
    "        pollClassificationInterval = setInterval(async () => {\n",
    "            try {\n",
    "                const response = await fetch('/webcam_classification');\n",
    "                if (!response.ok) {\n",
    "                    throw new Error('Failed to fetch classification data.');\n",
    "                }\n",
    "                const data = await response.json();\n",
    "\n",
    "                let resultHtml = '';\n",
    "                if (data.recyclable.length > 0) {\n",
    "                    resultHtml += `<div class=\"bg-yellow-200 p-4 rounded\"><strong>Recyclable items:</strong><br>- ${data.recyclable.join('<br>- ')}</div>`;\n",
    "                }\n",
    "                if (data.non_recyclable.length > 0) {\n",
    "                    resultHtml += `<div class=\"bg-blue-200 p-4 rounded\"><strong>Non-Recyclable items:</strong><br>- ${data.non_recyclable.join('<br>- ')}</div>`;\n",
    "                }\n",
    "                if (data.hazardous.length > 0) {\n",
    "                    resultHtml += `<div class=\"bg-red-200 p-4 rounded\"><strong>Hazardous items:</strong><br>- ${data.hazardous.join('<br>- ')}</div>`;\n",
    "                }\n",
    "                webcamResult.innerHTML = resultHtml;\n",
    "\n",
    "            } catch (error) {\n",
    "                console.error('Frontend: Error fetching webcam classification:', error);\n",
    "                // Only show error if the webcam stream itself isn't stopped\n",
    "                if (webcam.src !== '') {\n",
    "                    // Avoid spamming error messages if backend is just shutting down\n",
    "                    if (!error.message.includes(\"Failed to fetch\")) { // Simple check to avoid network errors during shutdown\n",
    "                        showMessage(webcamStatus, `Error fetching classification: ${error.message}`, 'error');\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }, 1000); // Poll every 1 second (adjust as needed)\n",
    "    });\n",
    "\n",
    "    stopWebcamBtn.addEventListener('click', async () => {\n",
    "        // First, tell the backend to stop the stream\n",
    "        showMessage(webcamStatus, 'Sending stop signal to backend...', 'info');\n",
    "        try {\n",
    "            const response = await fetch('/stop_webcam_backend', { \n",
    "                method: 'POST',\n",
    "                // Optional: set headers if needed, e.g., 'Content-Type': 'application/json' for body\n",
    "            });\n",
    "            if (!response.ok) {\n",
    "                // If backend returns an error status\n",
    "                const errorData = await response.json();\n",
    "                console.error('Frontend: Failed to send stop signal to backend:', errorData.message);\n",
    "                showMessage(webcamStatus, `Failed to stop webcam on backend: ${errorData.message}`, 'error');\n",
    "            } else {\n",
    "                console.log('Frontend: Stop signal sent to backend successfully.');\n",
    "                // Backend will handle releasing the camera now\n",
    "            }\n",
    "        } catch (error) {\n",
    "            // Network error (e.g., backend crashed, unreachable)\n",
    "            console.error('Frontend: Error sending stop signal:', error);\n",
    "            showMessage(webcamStatus, `Error sending stop signal: ${error.message}`, 'error');\n",
    "        } finally {\n",
    "            // Regardless of backend signal success, stop frontend display and polling\n",
    "            if (webcam.src !== '') { \n",
    "                webcam.src = ''; // Stop the video stream by clearing src\n",
    "                webcam.classList.add('hidden');\n",
    "                startWebcamBtn.classList.remove('hidden');\n",
    "                stopWebcamBtn.classList.add('hidden');\n",
    "                webcamResult.innerHTML = '';\n",
    "            }\n",
    "            if (pollClassificationInterval) {\n",
    "                clearInterval(pollClassificationInterval); // Stop polling\n",
    "                pollClassificationInterval = null;\n",
    "            }\n",
    "            // Give a moment for backend to actually release before showing 'stopped' status\n",
    "            setTimeout(() => {\n",
    "                showMessage(webcamStatus, 'Webcam stopped successfully.', 'success');\n",
    "            }, 1000); // Waktu yang cukup untuk backend melepaskan kamera (misal 1 detik)\n",
    "        }\n",
    "    });\n",
    "});\n",
    "\"\"\"\n",
    "\n",
    "# Write script.js to the static directory\n",
    "with open(BASE_DIR / 'static' / 'script.js', 'w') as f:\n",
    "    f.write(script_js_content)\n",
    "\n",
    "print('script.js created successfully at:', BASE_DIR / 'static' / 'script.js')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main.py created successfully at: d:\\ipynb\\main.py\n",
      "index.html created successfully at: d:\\ipynb\\static\\index.html\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries (ensure they are imported if this cell is run independently)\n",
    "from pathlib import Path\n",
    "\n",
    "# Assume BASE_DIR is already defined from previous cells or define it here if running standalone\n",
    "BASE_DIR = Path.cwd() # Adjust if your BASE_DIR is different\n",
    "\n",
    "# Create main.py (FastAPI app for best.pt)\n",
    "main_py_content = \"\"\"from fastapi import FastAPI, File, UploadFile, HTTPException\n",
    "from fastapi.responses import HTMLResponse, StreamingResponse, JSONResponse\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import settings\n",
    "import io\n",
    "from PIL import Image\n",
    "import base64\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "app.mount('/static', StaticFiles(directory='static'), name='static')\n",
    "\n",
    "model_path = Path(settings.DETECTION_MODEL)\n",
    "try:\n",
    "    model = YOLO(model_path)\n",
    "except Exception as ex:\n",
    "    raise Exception(f'Unable to load model. Check the specified path: {model_path} - {ex}')\n",
    "\n",
    "latest_webcam_results = {\n",
    "    'recyclable': [],\n",
    "    'non_recyclable': [],\n",
    "    'hazardous': []\n",
    "}\n",
    "\n",
    "webcam_stop_event = asyncio.Event()\n",
    "\n",
    "def classify_waste_type(detected_items):\n",
    "    recyclable_items = set(detected_items) & set(settings.RECYCLABLE)\n",
    "    non_recyclable_items = set(detected_items) & set(settings.NON_RECYCLABLE)\n",
    "    hazardous_items = set(detected_items) & set(settings.HAZARDOUS)\n",
    "    return recyclable_items, non_recyclable_items, hazardous_items\n",
    "\n",
    "def remove_dash_from_class_name(class_name):\n",
    "    return class_name.replace('_', ' ')\n",
    "\n",
    "@app.get('/', response_class=HTMLResponse)\n",
    "async def serve_index():\n",
    "    with open('static/index.html', 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "@app.post('/detect')\n",
    "async def detect_image(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        contents = await file.read()\n",
    "        image = Image.open(io.BytesIO(contents)).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        results = model.predict(image, conf=0.6, verbose=False)\n",
    "        names = model.names\n",
    "        detected_items = set([names[int(c)] for c in results[0].boxes.cls])\n",
    "\n",
    "        recyclable_items, non_recyclable_items, hazardous_items = classify_waste_type(detected_items)\n",
    "        \n",
    "        result_dict = {\n",
    "            'recyclable': [remove_dash_from_class_name(item) for item in recyclable_items],\n",
    "            'non_recyclable': [remove_dash_from_class_name(item) for item in non_recyclable_items],\n",
    "            'hazardous': [remove_dash_from_class_name(item) for item in hazardous_items]\n",
    "        }\n",
    "\n",
    "        res_plotted = results[0].plot()\n",
    "        _, buffer = cv2.imencode('.jpg', res_plotted)\n",
    "        encoded_image = base64.b64encode(buffer).decode('utf-8')\n",
    "\n",
    "        return {'results': result_dict, 'image': encoded_image}\n",
    "    except Exception as e:\n",
    "        print(f'Error during image detection: {e}')\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get('/video_feed')\n",
    "async def video_feed():\n",
    "    global latest_webcam_results\n",
    "    webcam_stop_event.clear()\n",
    "    print('Backend: Webcam stream started, stop event cleared.')\n",
    "\n",
    "    async def generate():\n",
    "        cap = cv2.VideoCapture(settings.WEBCAM_PATH)\n",
    "        if not cap.isOpened():\n",
    "            print('Backend: Error: Could not open webcam.')\n",
    "            # Corrected indentation for yield continuation lines\n",
    "            yield (b'--frame\\\\r\\\\n'\n",
    "                   b'Content-Type: image/jpeg\\\\r\\\\n\\\\r\\\\n' +\n",
    "                   cv2.imencode('.jpg', np.zeros((480, 640, 3), dtype=np.uint8))[1].tobytes() + b'\\\\r\\\\n')\n",
    "            return\n",
    "\n",
    "        print('Backend: Webcam opened successfully. Streaming frames...')\n",
    "        \n",
    "        prev_frame_time = 0\n",
    "        new_frame_time = 0\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                if webcam_stop_event.is_set():\n",
    "                    print('Backend: Webcam stop event detected. Breaking loop.')\n",
    "                    break\n",
    "\n",
    "                success, frame = cap.read()\n",
    "                if not success:\n",
    "                    print('Backend: Error: Failed to read frame from webcam. Breaking loop.')\n",
    "                    break\n",
    "                \n",
    "                new_frame_time = time.time()\n",
    "                fps = 1 / (new_frame_time - prev_frame_time)\n",
    "                prev_frame_time = new_frame_time\n",
    "                fps_text = f'FPS: {int(fps)}'\n",
    "\n",
    "                results = model.predict(frame, conf=0.6, verbose=False)\n",
    "                names = model.names\n",
    "                detected_items = set([names[int(c)] for c in results[0].boxes.cls])\n",
    "\n",
    "                recyclable_items, non_recyclable_items, hazardous_items = classify_waste_type(detected_items)\n",
    "                \n",
    "                latest_webcam_results['recyclable'] = [remove_dash_from_class_name(item) for item in recyclable_items]\n",
    "                latest_webcam_results['non_recyclable'] = [remove_dash_from_class_name(item) for item in non_recyclable_items]\n",
    "                latest_webcam_results['hazardous'] = [remove_dash_from_class_name(item) for item in hazardous_items]\n",
    "\n",
    "                res_plotted = results[0].plot()\n",
    "                cv2.putText(res_plotted, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "                _, buffer = cv2.imencode('.jpg', res_plotted)\n",
    "                frame_bytes = buffer.tobytes()\n",
    "\n",
    "                # Corrected indentation for yield continuation lines\n",
    "                yield (b'--frame\\\\r\\\\n'\n",
    "                       b'Content-Type: image/jpeg\\\\r\\\\n\\\\r\\\\n' + frame_bytes + b'\\\\r\\\\n')\n",
    "                \n",
    "                await asyncio.sleep(0.001)\n",
    "        finally:\n",
    "            if cap.isOpened():\n",
    "                cap.release()\n",
    "                print('Backend: Webcam released.')\n",
    "            else:\n",
    "                print('Backend: Webcam was not opened, nothing to release.')\n",
    "\n",
    "    return StreamingResponse(generate(), media_type='multipart/x-mixed-replace;boundary=frame')\n",
    "\n",
    "@app.post('/stop_webcam_backend')\n",
    "async def stop_webcam_backend():\n",
    "    webcam_stop_event.set()\n",
    "    print('Backend: Received stop signal from frontend. Event set.')\n",
    "    return JSONResponse(content={'message': 'Webcam stop signal sent.'})\n",
    "\n",
    "@app.get('/webcam_classification')\n",
    "async def get_webcam_classification():\n",
    "    return JSONResponse(content=latest_webcam_results)\n",
    "\"\"\"\n",
    "\n",
    "with open(BASE_DIR / 'main.py', 'w') as f:\n",
    "    f.write(main_py_content)\n",
    "\n",
    "print('main.py created successfully at:', BASE_DIR / 'main.py')\n",
    "\n",
    "# Create a basic index.html for the FastAPI app\n",
    "index_html_content = \"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Waste Detection</title>\n",
    "    <script src=\"https://cdn.tailwindcss.com\"></script>\n",
    "</head>\n",
    "<body class=\"bg-gray-100 font-sans\">\n",
    "    <div class=\"container mx-auto p-4\">\n",
    "        <h1 class=\"text-3xl font-bold text-center mb-4\">Intelligent Waste Segregation System</h1>\n",
    "        <p class=\"text-center mb-6\">Upload an image or use your device camera to detect waste types.</p>\n",
    "\n",
    "        <div class=\"flex flex-col md:flex-row gap-6\">\n",
    "            <div class=\"flex-1 bg-white p-6 rounded-lg shadow-md\">\n",
    "                <h2 class=\"text-xl font-semibold mb-4\">Upload Image</h2>\n",
    "                <div id=\"drop-area\" class=\"border-2 border-dashed border-gray-400 p-6 text-center cursor-pointer hover:border-blue-500 hover:bg-blue-50 transition\">\n",
    "                    <p class=\"text-gray-600\">Drag and drop an image here or click to upload</p>\n",
    "                    <input type=\"file\" id=\"file-input\" accept=\"image/*\" class=\"hidden\">\n",
    "                </div>\n",
    "                <div id=\"image-preview\" class=\"mt-4 hidden\">\n",
    "                    <p class=\"text-sm text-gray-600 mb-2\">Image Preview:</p>\n",
    "                    <img src=\"#\" alt=\"Image Preview\" class=\"w-full rounded max-h-64 object-contain\">\n",
    "                </div>\n",
    "                <button id=\"upload-btn\" class=\"mt-4 w-full bg-blue-500 text-white py-2 px-4 rounded hover:bg-blue-600\">Upload and Detect</button>\n",
    "                <div id=\"upload-status\" class=\"mt-4 text-center hidden\"></div>\n",
    "                <div id=\"result-image\" class=\"mt-4\"></div>\n",
    "                <div id=\"result-text\" class=\"mt-4 space-y-2\"></div>\n",
    "            </div>\n",
    "\n",
    "            <div class=\"flex-1 bg-white p-6 rounded-lg shadow-md\">\n",
    "                <h2 class=\"text-xl font-semibold mb-4\">Live Webcam Detection</h2>\n",
    "                <div class=\"flex gap-2\">\n",
    "                    <button id=\"start-webcam\" class=\"flex-1 bg-blue-500 text-white py-2 px-4 rounded hover:bg-blue-600\">Start Webcam</button>\n",
    "                    <button id=\"stop-webcam\" class=\"flex-1 bg-red-500 text-white py-2 px-4 rounded hover:bg-red-600 hidden\">Stop Webcam</button>\n",
    "                </div>\n",
    "                <div class=\"mt-4 flex flex-col md:flex-row gap-4\">\n",
    "                    <img id=\"webcam\" src=\"\" class=\"w-full md:w-2/3 rounded hidden\">\n",
    "                    <div id=\"webcam-result\" class=\"w-full md:w-1/3 space-y-2\"></div>\n",
    "                </div>\n",
    "                 <div id=\"webcam-status\" class=\"mt-4 text-center hidden\"></div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <script src=\"/static/script.js\"></script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "with open(BASE_DIR / 'static' / 'index.html', 'w') as f:\n",
    "    f.write(index_html_content)\n",
    "\n",
    "print('index.html created successfully at:', BASE_DIR / 'static' / 'index.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensuring FastAPI and Uvicorn are installed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastAPI and Uvicorn installation check complete.\n",
      "Changing current directory to: d:\\ipynb\n",
      "d:\\ipynb\n",
      "Model 'best.pt' found. Proceeding to start FastAPI server.\n",
      "Starting FastAPI server...\n",
      "FastAPI server is running at http://127.0.0.1:8000\n",
      "Open this URL in your browser to test the app.\n",
      "\n",
      "FastAPI server will run for 5 minutes. Press the stop button to terminate early.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import subprocess\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os # Import os for path checking\n",
    "\n",
    "# Assume BASE_DIR is already defined from previous cells or define it here if running standalone\n",
    "BASE_DIR = Path.cwd() # Adjust if your BASE_DIR is different\n",
    "\n",
    "# Ensure FastAPI and Uvicorn are installed\n",
    "print(\"Ensuring FastAPI and Uvicorn are installed...\")\n",
    "!pip install fastapi --quiet\n",
    "!pip install uvicorn --quiet\n",
    "print(\"FastAPI and Uvicorn installation check complete.\")\n",
    "\n",
    "# Change directory to BASE_DIR before starting FastAPI\n",
    "print(f\"Changing current directory to: {BASE_DIR}\")\n",
    "%cd {BASE_DIR}\n",
    "\n",
    "# --- Pre-check for best.pt model ---\n",
    "# This is crucial as main.py will try to load it immediately\n",
    "# Assuming settings.py defines DETECTION_MODEL correctly as BASE_DIR / 'weights' / 'best.pt'\n",
    "model_path_check = BASE_DIR / 'weights' / 'best.pt'\n",
    "if not model_path_check.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Error: Model 'best.pt' not found at {model_path_check}. \"\n",
    "        \"Please ensure the training cell completed successfully and 'best.pt' was moved to the 'weights' directory.\"\n",
    "    )\n",
    "print(f\"Model '{model_path_check.name}' found. Proceeding to start FastAPI server.\")\n",
    "\n",
    "# Start FastAPI server\n",
    "# This starts the server at http://127.0.0.1:8000\n",
    "# Access it in your browser to test image upload and webcam feed\n",
    "print('Starting FastAPI server...')\n",
    "# Capture stdout and stderr to debug potential Uvicorn errors\n",
    "fastapi_process = subprocess.Popen(\n",
    "    ['uvicorn', 'main:app', '--host', '0.0.0.0', '--port', '8000'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True, # Decode stdout/stderr as text\n",
    "    bufsize=1 # Line-buffered output\n",
    ")\n",
    "\n",
    "# Wait for the server to start\n",
    "time.sleep(5) # Give it a few seconds to initialize\n",
    "\n",
    "if fastapi_process.poll() is None:\n",
    "    print('FastAPI server is running at http://127.0.0.1:8000')\n",
    "    print('Open this URL in your browser to test the app.')\n",
    "else:\n",
    "    # If the process terminated, read its output for debugging\n",
    "    stdout, stderr = fastapi_process.communicate()\n",
    "    print(\"\\n--- FastAPI Server STDOUT ---\")\n",
    "    print(stdout)\n",
    "    print(\"\\n--- FastAPI Server STDERR ---\")\n",
    "    print(stderr)\n",
    "    raise RuntimeError(\n",
    "        'Failed to start FastAPI server. Check the output above for Uvicorn errors.'\n",
    "    )\n",
    "\n",
    "# Keep the server running for 5 minutes to allow testing\n",
    "# You can interrupt this cell (e.g., using the stop button in Jupyter) to stop the server manually.\n",
    "try:\n",
    "    print('\\nFastAPI server will run for 5 minutes. Press the stop button to terminate early.')\n",
    "    time.sleep(300)  # 5 minutes\n",
    "finally:\n",
    "    if fastapi_process.poll() is None: # Check if process is still running\n",
    "        print('Terminating FastAPI server...')\n",
    "        fastapi_process.terminate()\n",
    "        fastapi_process.wait(timeout=10) # Wait a bit for it to terminate cleanly\n",
    "        if fastapi_process.poll() is None:\n",
    "            print(\"FastAPI server did not terminate, killing it...\")\n",
    "            fastapi_process.kill() # Force kill if it's still running\n",
    "        print('FastAPI server stopped.')\n",
    "    else:\n",
    "        print('FastAPI server was already stopped or never started successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Convert best.pt to RKNN format\n",
    "# Run this cell only after successfully testing the FastAPI app with best.pt\n",
    "# Requires rknn-toolkit2 and a compatible environment\n",
    "\n",
    "# Create convert_pt_to_rknn.py\n",
    "convert_rknn_content = \"\"\"from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO('weights/best.pt')\n",
    "\n",
    "# Export to RKNN format\n",
    "model.export(format='rknn', name='rk3588')\n",
    "\"\"\"\n",
    "\n",
    "with open(BASE_DIR / 'convert_pt_to_rknn.py', 'w') as f:\n",
    "    f.write(convert_rknn_content)\n",
    "\n",
    "print('convert_pt_to_rknn.py created successfully.')\n",
    "\n",
    "# Run conversion\n",
    "# Note: This requires the RKNN toolkit and may need to be run on a compatible system\n",
    "# Comment out the next line if you're not ready to convert\n",
    "# %run convert_pt_to_rknn.py\n",
    "\n",
    "# Move RKNN model to weights directory\n",
    "# rknn_model_source = BASE_DIR / 'yolo11n_rknn_model' / 'best-rk3588.rknn'\n",
    "# rknn_model_dest = BASE_DIR / 'weights' / 'best-rk3588.rknn'\n",
    "# if rknn_model_source.exists():\n",
    "#     shutil.move(rknn_model_source, rknn_model_dest)\n",
    "#     print('RKNN model moved to weights directory.')\n",
    "# else:\n",
    "#     print('RKNN conversion did not produce the expected file. Check conversion logs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Create main1.py for RKNN deployment\n",
    "# This is the FastAPI app modified for RKNN on OrangePi5Pro\n",
    "# Run this cell to create the file, but execute it only on the OrangePi5Pro with the RKNN model\n",
    "\n",
    "main1_py_content = \"\"\"from fastapi import FastAPI, File, UploadFile, HTTPException\n",
    "from fastapi.responses import HTMLResponse, StreamingResponse, JSONResponse\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from rknn.api import RKNN\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import settings\n",
    "import io\n",
    "from PIL import Image\n",
    "import base64\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "app = FastAPI()\n",
    "app.mount('/static', StaticFiles(directory='static'), name='static')\n",
    "\n",
    "# Update settings to use RKNN model\n",
    "settings.DETECTION_MODEL = Path('weights/best-rk3588.rknn')\n",
    "\n",
    "# Initialize RKNN model\n",
    "model_path = Path(settings.DETECTION_MODEL)\n",
    "try:\n",
    "    rknn = RKNN(verbose=True)\n",
    "    model_path_str = str(model_path)\n",
    "    if rknn.load_rknn(model_path_str) != 0:\n",
    "        raise Exception(f'Failed to load RKNN model from {model_path_str}')\n",
    "    if rknn.init_runtime(target='rk3588') != 0:\n",
    "        raise Exception('Failed to initialize RKNN runtime')\n",
    "except Exception as ex:\n",
    "    raise Exception(f'Unable to load RKNN model. Check the specified path: {model_path} - {ex}')\n",
    "\n",
    "latest_webcam_results = {\n",
    "    'recyclable': [],\n",
    "    'non_recyclable': [],\n",
    "    'hazardous': []\n",
    "}\n",
    "\n",
    "webcam_stop_event = asyncio.Event()\n",
    "\n",
    "def classify_waste_type(detected_items):\n",
    "    recyclable_items = set(detected_items) & set(settings.RECYCLABLE)\n",
    "    non_recyclable_items = set(detected_items) & set(settings.NON_RECYCLABLE)\n",
    "    hazardous_items = set(detected_items) & set(settings.HAZARDOUS)\n",
    "    return recyclable_items, non_recyclable_items, hazardous_items\n",
    "\n",
    "def remove_dash_from_class_name(class_name):\n",
    "    return class_name.replace('_', ' ')\n",
    "\n",
    "def preprocess_image_for_rknn(image_np):\n",
    "    img_rgb = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
    "    original_h, original_w = img_rgb.shape[:2]\n",
    "    scale = min(settings.MODEL_INPUT_WIDTH / original_w, settings.MODEL_INPUT_HEIGHT / original_h)\n",
    "    new_w, new_h = int(original_w * scale), int(original_h * scale)\n",
    "    resized_img = cv2.resize(img_rgb, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "    padded_img = np.full((settings.MODEL_INPUT_HEIGHT, settings.MODEL_INPUT_WIDTH, 3), 128, dtype=np.uint8)\n",
    "    x_offset = (settings.MODEL_INPUT_WIDTH - new_w) // 2\n",
    "    y_offset = (settings.MODEL_INPUT_HEIGHT - new_h) // 2\n",
    "    padded_img[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized_img\n",
    "    input_data = np.expand_dims(padded_img, axis=0).astype(np.float32)\n",
    "    print(f'DEBUG PREPROCESS - Original: {original_w}x{original_h}, Scaled: {new_w}x{new_h}, Padded: {padded_img.shape}')\n",
    "    return input_data\n",
    "\n",
    "def postprocess_yolov8_rknn_output(rknn_outputs, original_img_shape):\n",
    "    print(f'DEBUG POSTPROCESS - RKNN outputs shapes: {[output.shape for output in rknn_outputs]}')\n",
    "    predictions = rknn_outputs[0]\n",
    "    predictions = predictions.transpose(0, 2, 1)\n",
    "    predictions = predictions[0]\n",
    "    print(f'DEBUG POSTPROCESS - Predictions shape after transpose: {predictions.shape}')\n",
    "\n",
    "    num_classes = len(settings.ALL_CLASSES)\n",
    "    img_h, img_w = original_img_shape\n",
    "    input_h, input_w = settings.MODEL_INPUT_HEIGHT, settings.MODEL_INPUT_WIDTH\n",
    "\n",
    "    boxes_raw = predictions[:, :4]\n",
    "    scores = predictions[:, 4:]\n",
    "\n",
    "    max_scores = np.max(scores, axis=1)\n",
    "    class_ids = np.argmax(scores, axis=1)\n",
    "\n",
    "    mask = max_scores >= settings.CONF_THRESHOLD\n",
    "    boxes = boxes_raw[mask]\n",
    "    max_scores = max_scores[mask]\n",
    "    class_ids = class_ids[mask]\n",
    "\n",
    "    if len(boxes) == 0:\n",
    "        print('No detections after confidence filtering.')\n",
    "        return [], [], [], original_img_shape\n",
    "\n",
    "    scale = min(input_w / img_w, input_h / img_h)\n",
    "    unpadded_w_in_model_coords = int(img_w * scale)\n",
    "    unpadded_h_in_model_coords = int(img_h * scale)\n",
    "    x_offset_in_model_coords = (input_w - unpadded_w_in_model_coords) // 2\n",
    "    y_offset_in_model_coords = (input_h - unpadded_h_in_model_coords) // 2\n",
    "\n",
    "    final_boxes_on_original = []\n",
    "    for box in boxes:\n",
    "        cx_padded, cy_padded, w_padded, h_padded = box\n",
    "        cx_unpadded = cx_padded - x_offset_in_model_coords\n",
    "        cy_unpadded = cy_padded - y_offset_in_model_coords\n",
    "        cx_original = cx_unpadded / scale\n",
    "        cy_original = cy_unpadded / scale\n",
    "        w_original = w_padded / scale\n",
    "        h_original = h_padded / scale\n",
    "        x1_original = cx_original - (w_original / 2)\n",
    "        y1_original = cy_original - (h_original / 2)\n",
    "        x2_original = cx_original + (w_original / 2)\n",
    "        y2_original = cy_original + (h_original / 2)\n",
    "        x1_original = np.clip(x1_original, 0, img_w)\n",
    "        y1_original = np.clip(y1_original, 0, img_h)\n",
    "        x2_original = np.clip(x2_original, 0, img_w)\n",
    "        y2_original = np.clip(y2_original, 0, img_h)\n",
    "        final_boxes_on_original.append([int(x1_original), int(y1_original), int(x2_original), int(y2_original)])\n",
    "\n",
    "    nms_boxes = [[x1, y1, x2-x1, y2-y1] for x1, y1, x2, y2 in final_boxes_on_original]\n",
    "    indices = []\n",
    "    if len(nms_boxes) > 0:\n",
    "        indices = cv2.dnn.NMSBoxes(nms_boxes, max_scores.tolist(), settings.CONF_THRESHOLD, settings.NMS_IOU_THRESHOLD)\n",
    "\n",
    "    final_boxes = []\n",
    "    final_confidences = []\n",
    "    final_class_ids = []\n",
    "\n",
    "    if len(indices) > 0:\n",
    "        indices = indices.flatten()\n",
    "        final_boxes = [final_boxes_on_original[i] for i in indices]\n",
    "        final_confidences = max_scores[indices]\n",
    "        final_class_ids = class_ids[indices]\n",
    "\n",
    "    detected_items = [settings.ALL_CLASSES[cls_id] for cls_id in final_class_ids if cls_id < num_classes]\n",
    "    print(f'DEBUG POSTPROCESS - Detected items: {detected_items}')\n",
    "    return final_boxes, final_confidences, final_class_ids, original_img_shape\n",
    "\n",
    "def draw_boxes_on_image(image_np, boxes, confidences, class_ids, fps_text=''):\n",
    "    image_np = image_np.copy()\n",
    "    names = settings.ALL_CLASSES\n",
    "    for i in range(len(boxes)):\n",
    "        x1, y1, x2, y2 = boxes[i]\n",
    "        confidence = confidences[i]\n",
    "        class_id = class_ids[i]\n",
    "        if class_id < len(names):\n",
    "            class_name = names[class_id]\n",
    "            color = (0, 255, 0)\n",
    "            text = f'{remove_dash_from_class_name(class_name)}: {confidence:.2f}'\n",
    "            cv2.rectangle(image_np, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(image_np, text, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    cv2.putText(image_np, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, False)\n",
    "    return image_np\n",
    "\n",
    "@app.get('/', response_class=HTMLResponse)\n",
    "async def serve_index():\n",
    "    with open('static/index.html', 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "@app.post('/detect')\n",
    "async def detect_image(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        contents = await file.read()\n",
    "        image = Image.open(io.BytesIO(contents)).convert('RGB')\n",
    "        image_np = np.array(image)\n",
    "        image_np = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
    "        original_shape = image_np.shape[:2]\n",
    "        input_image = preprocess_image_for_rknn(image_np)\n",
    "        outputs = rknn.inference(inputs=[input_image])\n",
    "        boxes, confidences, class_ids, _ = postprocess_yolov8_rknn_output(outputs, original_shape)\n",
    "        detected_items = [settings.ALL_CLASSES[cls_id] for cls_id in class_ids if cls_id < len(settings.ALL_CLASSES)]\n",
    "        recyclable_items, non_recyclable_items, hazardous_items = classify_waste_type(detected_items)\n",
    "        result_dict = {\n",
    "            'recyclable': [remove_dash_from_class_name(item) for item in recyclable_items],\n",
    "            'non_recyclable': [remove_dash_from_class_name(item) for item in non_recyclable_items],\n",
    "            'hazardous': [remove_dash_from_class_name(item) for item in hazardous_items]\n",
    "        }\n",
    "        plotted_image = draw_boxes_on_image(image_np, boxes, confidences, class_ids)\n",
    "        _, buffer = cv2.imencode('.jpg', plotted_image)\n",
    "        encoded_image = base64.b64encode(buffer).decode('utf-8')\n",
    "        return {'results': result_dict, 'image': encoded_image}\n",
    "    except Exception as e:\n",
    "        print(f'Error during image detection: {e}')\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get('/video_feed')\n",
    "async def video_feed():\n",
    "    global latest_webcam_results\n",
    "    webcam_stop_event.clear()\n",
    "    print('Backend: Webcam stream started, stop event cleared.')\n",
    "    async def generate():\n",
    "        cap = cv2.VideoCapture(settings.WEBCAM_PATH)\n",
    "        if not cap.isOpened():\n",
    "            print('Backend: Error: Could not open webcam.')\n",
    "            yield (b'--frame\\r\\n'\n",
    "                   b'Content-Type: image/jpeg\\r\\n\\r\\n' +\n",
    "                   cv2.imencode('.jpg', np.zeros((480, 640, 3), dtype=np.uint8))[1].tobytes() + b'\\r\\n')\n",
    "            return\n",
    "        print('Backend: Webcam opened successfully. Streaming frames...')\n",
    "        prev_frame_time = 0\n",
    "        new_frame_time = 0\n",
    "        try:\n",
    "            while True:\n",
    "                if webcam_stop_event.is_set():\n",
    "                    print('Backend: Webcam stop event detected. Breaking loop.')\n",
    "                    break\n",
    "                success, frame = cap.read()\n",
    "                if not success:\n",
    "                    print('Backend: Error: Failed to read frame from webcam. Breaking loop.')\n",
    "                    break\n",
    "                new_frame_time = time.time()\n",
    "                fps = 1 / (new_frame_time - prev_frame_time)\n",
    "                prev_frame_time = new_frame_time\n",
    "                fps_text = f'FPS: {int(fps)}'\n",
    "                original_shape = frame.shape[:2]\n",
    "                input_image = preprocess_image_for_rknn(frame)\n",
    "                outputs = rknn.inference(inputs=[input_image])\n",
    "                boxes, confidences, class_ids, _ = postprocess_yolov8_rknn_output(outputs, original_shape)\n",
    "                detected_items = [settings.ALL_CLASSES[cls_id] for cls_id in class_ids if cls_id < len(settings.ALL_CLASSES)]\n",
    "                recyclable_items, non_recyclable_items, hazardous_items = classify_waste_type(detected_items)\n",
    "                latest_webcam_results['recyclable'] = [remove_dash_from_class_name(item) for item in recyclable_items]\n",
    "                latest_webcam_results['non_recyclable'] = [remove_dash_from_class_name(item) for item in non_recyclable_items]\n",
    "                latest_webcam_results['hazardous'] = [remove_dash_from_class_name(item) for item in hazardous_items]\n",
    "                plotted_image = draw_boxes_on_image(frame, boxes, confidences, class_ids, fps_text)\n",
    "                _, buffer = cv2.imencode('.jpg', plotted_image)\n",
    "                frame_bytes = buffer.tobytes()\n",
    "                yield (b'--frame\\r\\n'\n",
    "                       b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame_bytes + b'\\r\\n')\n",
    "                await asyncio.sleep(0.001)\n",
    "        finally:\n",
    "            if cap.isOpened():\n",
    "                cap.release()\n",
    "                print('Backend: Webcam released.')\n",
    "            else:\n",
    "                print('Backend: Webcam was not opened, nothing to release.')\n",
    "    return StreamingResponse(generate(), media_type='multipart/x-mixed-replace;boundary=frame')\n",
    "\n",
    "@app.post('/stop_webcam_backend')\n",
    "async def stop_webcam_backend():\n",
    "    webcam_stop_event.set()\n",
    "    print('Backend: Received stop signal from frontend. Event set.')\n",
    "    return JSONResponse(content={'message': 'Webcam stop signal sent.'})\n",
    "\n",
    "@app.get('/webcam_classification')\n",
    "async def get_webcam_classification():\n",
    "    return JSONResponse(content=latest_webcam_results)\n",
    "\n",
    "@app.on_event('shutdown')\n",
    "def cleanup():\n",
    "    rknn.release()\n",
    "    print('Backend: RKNN model released.')\n",
    "\"\"\"\n",
    "\n",
    "with open(BASE_DIR / 'main1.py', 'w') as f:\n",
    "    f.write(main1_py_content)\n",
    "\n",
    "print('main1.py created successfully.')\n",
    "\n",
    "# Note: To run the RKNN FastAPI app, copy the following files to your OrangePi5Pro:\n",
    "# - weights/best-rk3588.rknn\n",
    "# - settings.py\n",
    "# - main1.py\n",
    "# - static/index.html\n",
    "# Then run: uvicorn main1:app --host 0.0.0.0 --port 8000\n",
    "# Ensure the RKNN toolkit and dependencies are installed on the OrangePi5Pro."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
