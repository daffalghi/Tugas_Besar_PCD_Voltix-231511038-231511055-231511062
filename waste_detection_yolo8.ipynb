{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waste Detection with YOLOv8 and FastAPI\n",
    "\n",
    "This notebook sets up and runs a waste detection system using YOLOv8 for training and inference, and FastAPI for serving the model. It includes steps to download the dataset from Roboflow, train the model, run the FastAPI app with `best.pt`, and optionally convert the model to RKNN format for deployment on OrangePi5Pro.\n",
    "\n",
    "**Dataset:**\n",
    "- The dataset is sourced from Roboflow Universe: [Waste Detection Dataset](https://universe.roboflow.com/ai-project-i3wje/waste-detection-vqkjo).\n",
    "- It contains 9178 images of recyclable and non-recyclable waste, annotated for object detection with 22 classes.\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3.8-3.11 with dependencies listed in `requirements.txt`.\n",
    "- A Roboflow API key to download the dataset (sign up at [Roboflow](https://roboflow.com) to get one).\n",
    "- GPU (optional but recommended for training).\n",
    "- Webcam (for testing FastAPI video feed).\n",
    "\n",
    "**Steps:**\n",
    "1. Download and set up the dataset from Roboflow.\n",
    "2. Train the YOLOv8 model.\n",
    "3. Run the FastAPI app with `best.pt`.\n",
    "4. (Optional) Convert `best.pt` to RKNN and run FastAPI with RKNN model.\n",
    "\n",
    "**Dataset Setup Tutorial:**\n",
    "1. **Obtain Roboflow API Key:**\n",
    "   - Sign up or log in at [Roboflow](https://roboflow.com).\n",
    "   - Navigate to your account settings to find your API key.\n",
    "   - Provide this key when prompted in the dataset download cell below.\n",
    "\n",
    "2. **Download the Dataset:**\n",
    "   - The notebook will download the dataset using the Roboflow API to the `dataset` directory.\n",
    "   - The dataset will be in YOLO format, with `train`, `valid`, and `test` folders, each containing `images` and `labels` subfolders.\n",
    "   - Alternatively, you can manually download the dataset from [https://universe.roboflow.com/ai-project-i3wje/waste-detection-vqkjo](https://universe.roboflow.com/ai-project-i3wje/waste-detection-vqkjo) in YOLOv8 format and extract it to the `dataset` directory.\n",
    "\n",
    "3. **Place the Dataset Folders:**\n",
    "   - After downloading, the dataset will be automatically placed in the `dataset` directory with the following structure:\n",
    "     ```\n",
    "     dataset/\n",
    "     ├── train/\n",
    "     │   ├── images/\n",
    "     │   │   ├── image1.jpg\n",
    "     │   │   ├── image2.jpg\n",
    "     │   │   └── ...\n",
    "     │   └── labels/\n",
    "     │       ├── image1.txt\n",
    "     │       ├── image2.txt\n",
    "     │       └── ...\n",
    "     ├── valid/\n",
    "     │   ├── images/\n",
    "     │   └── labels/\n",
    "     ├── test/\n",
    "     │   ├── images/\n",
    "     │   └── labels/\n",
    "     └── data.yaml\n",
    "     ```\n",
    "   - Ensure the `data.yaml` file is in the `dataset` directory, as it specifies the paths to `train`, `valid`, and `test` images and the 22 class names.\n",
    "   - Do not modify the folder structure or `data.yaml` unless you know what you're doing, as the training script relies on this setup.\n",
    "\n",
    "4. **Verify the Dataset:**\n",
    "   - The notebook includes a verification step to check that the `train`, `valid`, and `test` directories contain images.\n",
    "   - If you encounter errors, ensure the dataset was downloaded correctly and the folders are populated.\n",
    "\n",
    "Run all cells sequentially to execute the pipeline. If you already have a trained `best.pt`, place it in the `weights` directory and skip the training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Roboflow library if not already installed\n",
    "!pip install ultralytics\n",
    "!pip install torch\n",
    "!pip install onnxruntime\n",
    "!pip install roboflow\n",
    "\n",
    "# Import necessary libraries for file handling\n",
    "import os\n",
    "from pathlib import Path\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# Define base directory\n",
    "BASE_DIR = Path.cwd()\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(BASE_DIR / 'dataset', exist_ok=True)\n",
    "os.makedirs(BASE_DIR / 'weights', exist_ok=True)\n",
    "os.makedirs(BASE_DIR / 'static', exist_ok=True)\n",
    "\n",
    "!pip install roboflow --quiet\n",
    "# Download dataset from Roboflow\n",
    "%cd {BASE_DIR}/datasets\n",
    "rf = Roboflow(api_key=\"HI42QRXkU9xSlq7DHhks\")\n",
    "project = rf.workspace(\"ai-project-i3wje\").project(\"waste-detection-vqkjo\")\n",
    "dataset = project.version(9).download(\"yolov8\")\n",
    "\n",
    "# Define dataset paths\n",
    "DATASET_DIR = BASE_DIR / 'dataset'\n",
    "TRAIN_IMAGES = DATASET_DIR / 'train' / 'images'\n",
    "VAL_IMAGES = DATASET_DIR / 'valid' / 'images'\n",
    "TEST_IMAGES = DATASET_DIR / 'test' / 'images'\n",
    "\n",
    "print('Dataset downloaded and directory structure verified.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data.yaml exists and is correctly formatted\n",
    "data_yaml_path = DATASET_DIR / 'data.yaml'\n",
    "if not data_yaml_path.exists():\n",
    "    raise FileNotFoundError('data.yaml not found in dataset directory. Ensure dataset downloaded correctly.')\n",
    "\n",
    "# Read and verify data.yaml content\n",
    "with open(data_yaml_path, 'r') as f:\n",
    "    data_yaml_content = f.read()\n",
    "\n",
    "print('data.yaml content:')\n",
    "print(data_yaml_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train.py\n",
    "train_py_content = \"\"\"from ultralytics import YOLO\n",
    "\n",
    "def main():\n",
    "    # Define path to data.yaml\n",
    "    data_yaml = str(Path.cwd() / 'dataset' / 'data.yaml')\n",
    "\n",
    "    # Load YOLOv8n model (pre-trained)\n",
    "    model = YOLO('yolov8n.pt')\n",
    "\n",
    "    # Training\n",
    "    print('Starting training...')\n",
    "    train_results = model.train(\n",
    "        data=data_yaml,\n",
    "        epochs=50,\n",
    "        imgsz=640,\n",
    "        batch=16,\n",
    "        device=0 if torch.cuda.is_available() else 'cpu',\n",
    "        name='waste_detection'\n",
    "    )\n",
    "\n",
    "    # Validation\n",
    "    print('Starting validation...')\n",
    "    val_results = model.val(\n",
    "        data=data_yaml,\n",
    "        imgsz=640,\n",
    "        batch=16,\n",
    "        device=0 if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "\n",
    "    # Prediction on test data\n",
    "    print('Starting prediction...')\n",
    "    predict_results = model.predict(\n",
    "        source=str(Path.cwd() / 'dataset' / 'test' / 'images'),\n",
    "        save=True,\n",
    "        imgsz=640,\n",
    "        device=0 if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "\n",
    "    print('Process completed!')\n",
    "    print(f'Training results saved at: runs/train/waste_detection')\n",
    "    print(f'Best model: runs/train/waste_detection/weights/best.pt')\n",
    "    print(f'Prediction results saved at: runs/predict/')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import torch\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "with open(BASE_DIR / 'train.py', 'w') as f:\n",
    "    f.write(train_py_content)\n",
    "\n",
    "print('train.py created successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create settings.py\n",
    "settings_py_content = \"\"\"from pathlib import Path\n",
    "import sys\n",
    "\n",
    "file_path = Path(__file__).resolve()\n",
    "root_path = file_path.parent\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(str(root_path))\n",
    "ROOT = root_path.relative_to(Path.cwd())\n",
    "\n",
    "# ML Model config\n",
    "MODEL_DIR = ROOT / 'weights'\n",
    "DETECTION_MODEL = MODEL_DIR / 'best.pt'\n",
    "\n",
    "MODEL_INPUT_WIDTH = 640\n",
    "MODEL_INPUT_HEIGHT = 640\n",
    "CONF_THRESHOLD = 0.25\n",
    "NMS_IOU_THRESHOLD = 0.45\n",
    "\n",
    "# Class names\n",
    "ALL_CLASSES = [\n",
    "    'battery', 'can', 'cardboard_bowl', 'cardboard_box', 'chemical_plastic_bottle',\n",
    "    'chemical_plastic_gallon', 'chemical_spray_can', 'light_bulb', 'paint_bucket',\n",
    "    'plastic_bag', 'plastic_bottle', 'plastic_bottle_cap', 'plastic_box',\n",
    "    'plastic_cultery', 'plastic_cup', 'plastic_cup_lid', 'reuseable_paper',\n",
    "    'scrap_paper', 'scrap_plastic', 'snack_bag', 'stick', 'straw'\n",
    "]\n",
    "\n",
    "# Webcam\n",
    "WEBCAM_PATH = 0\n",
    "\n",
    "# Waste type classification\n",
    "RECYCLABLE = ['cardboard_box', 'can', 'plastic_bottle_cap', 'plastic_bottle', 'reuseable_paper']\n",
    "NON_RECYCLABLE = [\n",
    "    'plastic_bag', 'scrap_paper', 'stick', 'plastic_cup', 'snack_bag',\n",
    "    'plastic_box', 'straw', 'plastic_cup_lid', 'scrap_plastic',\n",
    "    'cardboard_bowl', 'plastic_cultery'\n",
    "]\n",
    "HAZARDOUS = [\n",
    "    'battery', 'chemical_spray_can', 'chemical_plastic_bottle',\n",
    "    'chemical_plastic_gallon', 'light_bulb', 'paint_bucket'\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "with open(BASE_DIR / 'settings.py', 'w') as f:\n",
    "    f.write(settings_py_content)\n",
    "\n",
    "print('settings.py created successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create main.py (FastAPI app for best.pt)\n",
    "main_py_content = \"\"\"from fastapi import FastAPI, File, UploadFile, HTTPException\n",
    "from fastapi.responses import HTMLResponse, StreamingResponse, JSONResponse\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import settings\n",
    "import io\n",
    "from PIL import Image\n",
    "import base64\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "app.mount('/static', StaticFiles(directory='static'), name='static')\n",
    "\n",
    "model_path = Path(settings.DETECTION_MODEL)\n",
    "try:\n",
    "    model = YOLO(model_path)\n",
    "except Exception as ex:\n",
    "    raise Exception(f'Unable to load model. Check the specified path: {model_path} - {ex}')\n",
    "\n",
    "latest_webcam_results = {\n",
    "    'recyclable': [],\n",
    "    'non_recyclable': [],\n",
    "    'hazardous': []\n",
    "}\n",
    "\n",
    "webcam_stop_event = asyncio.Event()\n",
    "\n",
    "def classify_waste_type(detected_items):\n",
    "    recyclable_items = set(detected_items) & set(settings.RECYCLABLE)\n",
    "    non_recyclable_items = set(detected_items) & set(settings.NON_RECYCLABLE)\n",
    "    hazardous_items = set(detected_items) & set(settings.HAZARDOUS)\n",
    "    return recyclable_items, non_recyclable_items, hazardous_items\n",
    "\n",
    "def remove_dash_from_class_name(class_name):\n",
    "    return class_name.replace('_', ' ')\n",
    "\n",
    "@app.get('/', response_class=HTMLResponse)\n",
    "async def serve_index():\n",
    "    with open('static/index.html', 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "@app.post('/detect')\n",
    "async def detect_image(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        contents = await file.read()\n",
    "        image = Image.open(io.BytesIO(contents)).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        results = model.predict(image, conf=0.6, verbose=False)\n",
    "        names = model.names\n",
    "        detected_items = set([names[int(c)] for c in results[0].boxes.cls])\n",
    "\n",
    "        recyclable_items, non_recyclable_items, hazardous_items = classify_waste_type(detected_items)\n",
    "        \n",
    "        result_dict = {\n",
    "            'recyclable': [remove_dash_from_class_name(item) for item in recyclable_items],\n",
    "            'non_recyclable': [remove_dash_from_class_name(item) for item in non_recyclable_items],\n",
    "            'hazardous': [remove_dash_from_class_name(item) for item in hazardous_items]\n",
    "        }\n",
    "\n",
    "        res_plotted = results[0].plot()\n",
    "        _, buffer = cv2.imencode('.jpg', res_plotted)\n",
    "        encoded_image = base64.b64encode(buffer).decode('utf-8')\n",
    "\n",
    "        return {'results': result_dict, 'image': encoded_image}\n",
    "    except Exception as e:\n",
    "        print(f'Error during image detection: {e}')\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get('/video_feed')\n",
    "async def video_feed():\n",
    "    global latest_webcam_results\n",
    "    webcam_stop_event.clear()\n",
    "    print('Backend: Webcam stream started, stop event cleared.')\n",
    "\n",
    "    async def generate():\n",
    "        cap = cv2.VideoCapture(settings.WEBCAM_PATH)\n",
    "        if not cap.isOpened():\n",
    "            print('Backend: Error: Could not open webcam.')\n",
    "            yield (b'--frame\\r\\n'\n",
    "                   b'Content-Type: image/jpeg\\r\\n\\r\\n' +\n",
    "                   cv2.imencode('.jpg', np.zeros((480, 640, 3), dtype=np.uint8))[1].tobytes() + b'\\r\\n')\n",
    "            return\n",
    "\n",
    "        print('Backend: Webcam opened successfully. Streaming frames...')\n",
    "        \n",
    "        prev_frame_time = 0\n",
    "        new_frame_time = 0\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                if webcam_stop_event.is_set():\n",
    "                    print('Backend: Webcam stop event detected. Breaking loop.')\n",
    "                    break\n",
    "\n",
    "                success, frame = cap.read()\n",
    "                if not success:\n",
    "                    print('Backend: Error: Failed to read frame from webcam. Breaking loop.')\n",
    "                    break\n",
    "                \n",
    "                new_frame_time = time.time()\n",
    "                fps = 1 / (new_frame_time - prev_frame_time)\n",
    "                prev_frame_time = new_frame_time\n",
    "                fps_text = f'FPS: {int(fps)}'\n",
    "\n",
    "                results = model.predict(frame, conf=0.6, verbose=False)\n",
    "                names = model.names\n",
    "                detected_items = set([names[int(c)] for c in results[0].boxes.cls])\n",
    "\n",
    "                recyclable_items, non_recyclable_items, hazardous_items = classify_waste_type(detected_items)\n",
    "                \n",
    "                latest_webcam_results['recyclable'] = [remove_dash_from_class_name(item) for item in recyclable_items]\n",
    "                latest_webcam_results['non_recyclable'] = [remove_dash_from_class_name(item) for item in non_recyclable_items]\n",
    "                latest_webcam_results['hazardous'] = [remove_dash_from_class_name(item) for item in hazardous_items]\n",
    "\n",
    "                res_plotted = results[0].plot()\n",
    "                cv2.putText(res_plotted, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "                _, buffer = cv2.imencode('.jpg', res_plotted)\n",
    "                frame_bytes = buffer.tobytes()\n",
    "\n",
    "                yield (b'--frame\\r\\n'\n",
    "                       b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame_bytes + b'\\r\\n')\n",
    "                \n",
    "                await asyncio.sleep(0.001)\n",
    "        finally:\n",
    "            if cap.isOpened():\n",
    "                cap.release()\n",
    "                print('Backend: Webcam released.')\n",
    "            else:\n",
    "                print('Backend: Webcam was not opened, nothing to release.')\n",
    "\n",
    "    return StreamingResponse(generate(), media_type='multipart/x-mixed-replace;boundary=frame')\n",
    "\n",
    "@app.post('/stop_webcam_backend')\n",
    "async def stop_webcam_backend():\n",
    "    webcam_stop_event.set()\n",
    "    print('Backend: Received stop signal from frontend. Event set.')\n",
    "    return JSONResponse(content={'message': 'Webcam stop signal sent.'})\n",
    "\n",
    "@app.get('/webcam_classification')\n",
    "async def get_webcam_classification():\n",
    "    return JSONResponse(content=latest_webcam_results)\n",
    "\"\"\"\n",
    "\n",
    "with open(BASE_DIR / 'main.py', 'w') as f:\n",
    "    f.write(main_py_content)\n",
    "\n",
    "print('main.py created successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a basic index.html for the FastAPI app\n",
    "index_html_content = \"\"\"<!DOCTYPE html>\n",
    "<html lang='en'>\n",
    "<head>\n",
    "    <meta charset='UTF-8'>\n",
    "    <meta name='viewport' content='width=device-width, initial-scale=1.0'>\n",
    "    <title>Waste Detection</title>\n",
    "    <style>\n",
    "        body { font-family: Arial, sans-serif; text-align: center; }\n",
    "        #videoFeed { max-width: 100%; }\n",
    "       Här{2} results { margin-top: 20px; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Waste Detection System</h1>\n",
    "    <h2>Upload Image</h2>\n",
    "    <input type='file' id='imageUpload' accept='image/*'>\n",
    "    <button onclick='submitImage()'>Detect</button>\n",
    "    <h2>Webcam Feed</h2>\n",
    "    <img id='videoFeed' src='/video_feed' alt='Webcam Feed'>\n",
    "    <button onclick='stopWebcam()'>Stop Webcam</button>\n",
    "    <div id='results'></div>\n",
    "\n",
    "    <script>\n",
    "        async function submitImage() {\n",
    "            const fileInput = document.getElementById('imageUpload');\n",
    "            const file = fileInput.files[0];\n",
    "            if (!file) {\n",
    "                alert('Please select an image file.');\n",
    "                return;\n",
    "            }\n",
    "\n",
    "            const formData = new FormData();\n",
    "            formData.append('file', file);\n",
    "\n",
    "            try {\n",
    "                const response = await fetch('/detect', {\n",
    "                    method: 'POST',\n",
    "                    body: formData\n",
    "                });\n",
    "                const result = await response.json();\n",
    "\n",
    "                document.getElementById('results').innerHTML = `\n",
    "                    <h3>Detection Results:</h3>\n",
    "                    <p>Recyclable Items: ${result.results.recyclable_items.join(', ') || 'None'}</p>\n",
    "                    <p>Non-Recyclable Items: ${result.results.non_recyclable_items.join(', ') || 'None'}</p>\n",
    "                    <p>Hazardous Items: ${result.results.hazardous_items.join(', ') || 'None'}</p>\n",
    "                    <img src='data:image/jpeg;base64,${result.image}' style='max-width: 100%;'>\n",
    "                `;\n",
    "            } catch (error) {\n",
    "                console.error('Error uploading image:', error);\n",
    "                alert('Image detection failed. Please try again.');\n",
    "            }\n",
    "        }\n",
    "\n",
    "        async function stopWebcam() {\n",
    "            try {\n",
    "                await fetch('/stop_webcam', {\n",
    "                    method: 'POST'\n",
    "                });\n",
    "                document.getElementById('videoFeed').src = '';\n",
    "            } catch (error) {\n",
    "                console.error('Error stopping webcam:', error);\n",
    "            }\n",
    "        }\n",
    "\n",
    "        async function updateClassification() {\n",
    "            try {\n",
    "                const response = await fetch('/webcam_classification');\n",
    "                const result = await response.json();\n",
    "                document.getElementById('results').innerHTML = `\n",
    "                    <h3>Webcam Classification Results:</h3>\n",
    "                    <p>Recyclable Items: ${result.recyclable_items.join(', ') || 'None'}</p>\n",
    "                    <p>Non-Recyclable Items: ${result.non_recyclable_items.join(', ') || 'None'}</p>\n",
    "                    <p>Hazardous Items: ${result.hazardous_items.join(', ') || 'None'}</p>\n",
    "                `;\n",
    "            } catch (error) {\n",
    "                console.error('Error fetching classification:', error);\n",
    "            }\n",
    "            setTimeout(updateClassification, 1000);\n",
    "        }\n",
    "\n",
    "        updateClassification();\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "with open(BASE_DIR / 'static' / 'index.html', 'w') as f:\n",
    "    f.write(index_html_content)\n",
    "\n",
    "print('index.html created successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset existence\n",
    "if not any(TRAIN_IMAGES.iterdir()) or not any(VAL_IMAGES.iterdir()) or not any(TEST_IMAGES.iterdir()):\n",
    "    raise FileNotFoundError('Dataset directories (train/images, valid/images, test/images) are empty. Ensure dataset was downloaded correctly.')\n",
    "\n",
    "print('Dataset directories verified.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "# Note: This step may take significant time depending on your hardware and dataset size.\n",
    "# If you already have a trained 'best.pt', you can skip this cell and copy 'best.pt' to the 'weights' directory.\n",
    "\n",
    "%run train.py\n",
    "\n",
    "# Move best.pt to weights directory\n",
    "import shutil\n",
    "\n",
    "best_pt_source = BASE_DIR / 'runs' / 'train' / 'waste_detection' / 'weights' / 'best.pt'\n",
    "best_pt_dest = BASE_DIR / 'weights' / 'best.pt'\n",
    "\n",
    "if best_pt_source.exists():\n",
    "    shutil.move(best_pt_source, best_pt_dest)\n",
    "    print('best.pt moved to weights directory.')\n",
    "else:\n",
    "    raise FileNotFoundError('Training did not produce best.pt. Check training logs for errors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run FastAPI app in the background\n",
    "# This starts the server at http://127.0.0.1:8000\n",
    "# Access it in your browser to test image upload and webcam feed\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Start FastAPI server\n",
    "fastapi_process = subprocess.Popen(['uvicorn', 'main:app', '--host', '0.0.0.0', '--port', '8000'])\n",
    "\n",
    "# Wait for the server to start\n",
    "time.sleep(5)\n",
    "\n",
    "if fastapi_process.poll() is None:\n",
    "    print('FastAPI server is running at http://127.0.0.1:8000')\n",
    "    print('Open this URL in your browser to test the app.')\n",
    "else:\n",
    "    raise RuntimeError('Failed to start FastAPI server. Check for errors above.')\n",
    "\n",
    "# Keep the server running for 5 minutes to allow testing\n",
    "# You can interrupt this cell to stop the server\n",
    "try:\n",
    "    time.sleep(300)  # 5 minutes\n",
    "finally:\n",
    "    fastapi_process.terminate()\n",
    "    print('FastAPI server stopped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Convert best.pt to RKNN format\n",
    "# Run this cell only after successfully testing the FastAPI app with best.pt\n",
    "# Requires rknn-toolkit2 and a compatible environment\n",
    "\n",
    "# Create convert_pt_to_rknn.py\n",
    "convert_rknn_content = \"\"\"from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO('weights/best.pt')\n",
    "\n",
    "# Export to RKNN format\n",
    "model.export(format='rknn', name='rk3588')\n",
    "\"\"\"\n",
    "\n",
    "with open(BASE_DIR / 'convert_pt_to_rknn.py', 'w') as f:\n",
    "    f.write(convert_rknn_content)\n",
    "\n",
    "print('convert_pt_to_rknn.py created successfully.')\n",
    "\n",
    "# Run conversion\n",
    "# Note: This requires the RKNN toolkit and may need to be run on a compatible system\n",
    "# Comment out the next line if you're not ready to convert\n",
    "# %run convert_pt_to_rknn.py\n",
    "\n",
    "# Move RKNN model to weights directory\n",
    "# rknn_model_source = BASE_DIR / 'yolo11n_rknn_model' / 'best-rk3588.rknn'\n",
    "# rknn_model_dest = BASE_DIR / 'weights' / 'best-rk3588.rknn'\n",
    "# if rknn_model_source.exists():\n",
    "#     shutil.move(rknn_model_source, rknn_model_dest)\n",
    "#     print('RKNN model moved to weights directory.')\n",
    "# else:\n",
    "#     print('RKNN conversion did not produce the expected file. Check conversion logs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Create main1.py for RKNN deployment\n",
    "# This is the FastAPI app modified for RKNN on OrangePi5Pro\n",
    "# Run this cell to create the file, but execute it only on the OrangePi5Pro with the RKNN model\n",
    "\n",
    "main1_py_content = \"\"\"from fastapi import FastAPI, File, UploadFile, HTTPException\n",
    "from fastapi.responses import HTMLResponse, StreamingResponse, JSONResponse\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from rknn.api import RKNN\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import settings\n",
    "import io\n",
    "from PIL import Image\n",
    "import base64\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "app = FastAPI()\n",
    "app.mount('/static', StaticFiles(directory='static'), name='static')\n",
    "\n",
    "# Update settings to use RKNN model\n",
    "settings.DETECTION_MODEL = Path('weights/best-rk3588.rknn')\n",
    "\n",
    "# Initialize RKNN model\n",
    "model_path = Path(settings.DETECTION_MODEL)\n",
    "try:\n",
    "    rknn = RKNN(verbose=True)\n",
    "    model_path_str = str(model_path)\n",
    "    if rknn.load_rknn(model_path_str) != 0:\n",
    "        raise Exception(f'Failed to load RKNN model from {model_path_str}')\n",
    "    if rknn.init_runtime(target='rk3588') != 0:\n",
    "        raise Exception('Failed to initialize RKNN runtime')\n",
    "except Exception as ex:\n",
    "    raise Exception(f'Unable to load RKNN model. Check the specified path: {model_path} - {ex}')\n",
    "\n",
    "latest_webcam_results = {\n",
    "    'recyclable': [],\n",
    "    'non_recyclable': [],\n",
    "    'hazardous': []\n",
    "}\n",
    "\n",
    "webcam_stop_event = asyncio.Event()\n",
    "\n",
    "def classify_waste_type(detected_items):\n",
    "    recyclable_items = set(detected_items) & set(settings.RECYCLABLE)\n",
    "    non_recyclable_items = set(detected_items) & set(settings.NON_RECYCLABLE)\n",
    "    hazardous_items = set(detected_items) & set(settings.HAZARDOUS)\n",
    "    return recyclable_items, non_recyclable_items, hazardous_items\n",
    "\n",
    "def remove_dash_from_class_name(class_name):\n",
    "    return class_name.replace('_', ' ')\n",
    "\n",
    "def preprocess_image_for_rknn(image_np):\n",
    "    img_rgb = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
    "    original_h, original_w = img_rgb.shape[:2]\n",
    "    scale = min(settings.MODEL_INPUT_WIDTH / original_w, settings.MODEL_INPUT_HEIGHT / original_h)\n",
    "    new_w, new_h = int(original_w * scale), int(original_h * scale)\n",
    "    resized_img = cv2.resize(img_rgb, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "    padded_img = np.full((settings.MODEL_INPUT_HEIGHT, settings.MODEL_INPUT_WIDTH, 3), 128, dtype=np.uint8)\n",
    "    x_offset = (settings.MODEL_INPUT_WIDTH - new_w) // 2\n",
    "    y_offset = (settings.MODEL_INPUT_HEIGHT - new_h) // 2\n",
    "    padded_img[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized_img\n",
    "    input_data = np.expand_dims(padded_img, axis=0).astype(np.float32)\n",
    "    print(f'DEBUG PREPROCESS - Original: {original_w}x{original_h}, Scaled: {new_w}x{new_h}, Padded: {padded_img.shape}')\n",
    "    return input_data\n",
    "\n",
    "def postprocess_yolov8_rknn_output(rknn_outputs, original_img_shape):\n",
    "    print(f'DEBUG POSTPROCESS - RKNN outputs shapes: {[output.shape for output in rknn_outputs]}')\n",
    "    predictions = rknn_outputs[0]\n",
    "    predictions = predictions.transpose(0, 2, 1)\n",
    "    predictions = predictions[0]\n",
    "    print(f'DEBUG POSTPROCESS - Predictions shape after transpose: {predictions.shape}')\n",
    "\n",
    "    num_classes = len(settings.ALL_CLASSES)\n",
    "    img_h, img_w = original_img_shape\n",
    "    input_h, input_w = settings.MODEL_INPUT_HEIGHT, settings.MODEL_INPUT_WIDTH\n",
    "\n",
    "    boxes_raw = predictions[:, :4]\n",
    "    scores = predictions[:, 4:]\n",
    "\n",
    "    max_scores = np.max(scores, axis=1)\n",
    "    class_ids = np.argmax(scores, axis=1)\n",
    "\n",
    "    mask = max_scores >= settings.CONF_THRESHOLD\n",
    "    boxes = boxes_raw[mask]\n",
    "    max_scores = max_scores[mask]\n",
    "    class_ids = class_ids[mask]\n",
    "\n",
    "    if len(boxes) == 0:\n",
    "        print('No detections after confidence filtering.')\n",
    "        return [], [], [], original_img_shape\n",
    "\n",
    "    scale = min(input_w / img_w, input_h / img_h)\n",
    "    unpadded_w_in_model_coords = int(img_w * scale)\n",
    "    unpadded_h_in_model_coords = int(img_h * scale)\n",
    "    x_offset_in_model_coords = (input_w - unpadded_w_in_model_coords) // 2\n",
    "    y_offset_in_model_coords = (input_h - unpadded_h_in_model_coords) // 2\n",
    "\n",
    "    final_boxes_on_original = []\n",
    "    for box in boxes:\n",
    "        cx_padded, cy_padded, w_padded, h_padded = box\n",
    "        cx_unpadded = cx_padded - x_offset_in_model_coords\n",
    "        cy_unpadded = cy_padded - y_offset_in_model_coords\n",
    "        cx_original = cx_unpadded / scale\n",
    "        cy_original = cy_unpadded / scale\n",
    "        w_original = w_padded / scale\n",
    "        h_original = h_padded / scale\n",
    "        x1_original = cx_original - (w_original / 2)\n",
    "        y1_original = cy_original - (h_original / 2)\n",
    "        x2_original = cx_original + (w_original / 2)\n",
    "        y2_original = cy_original + (h_original / 2)\n",
    "        x1_original = np.clip(x1_original, 0, img_w)\n",
    "        y1_original = np.clip(y1_original, 0, img_h)\n",
    "        x2_original = np.clip(x2_original, 0, img_w)\n",
    "        y2_original = np.clip(y2_original, 0, img_h)\n",
    "        final_boxes_on_original.append([int(x1_original), int(y1_original), int(x2_original), int(y2_original)])\n",
    "\n",
    "    nms_boxes = [[x1, y1, x2-x1, y2-y1] for x1, y1, x2, y2 in final_boxes_on_original]\n",
    "    indices = []\n",
    "    if len(nms_boxes) > 0:\n",
    "        indices = cv2.dnn.NMSBoxes(nms_boxes, max_scores.tolist(), settings.CONF_THRESHOLD, settings.NMS_IOU_THRESHOLD)\n",
    "\n",
    "    final_boxes = []\n",
    "    final_confidences = []\n",
    "    final_class_ids = []\n",
    "\n",
    "    if len(indices) > 0:\n",
    "        indices = indices.flatten()\n",
    "        final_boxes = [final_boxes_on_original[i] for i in indices]\n",
    "        final_confidences = max_scores[indices]\n",
    "        final_class_ids = class_ids[indices]\n",
    "\n",
    "    detected_items = [settings.ALL_CLASSES[cls_id] for cls_id in final_class_ids if cls_id < num_classes]\n",
    "    print(f'DEBUG POSTPROCESS - Detected items: {detected_items}')\n",
    "    return final_boxes, final_confidences, final_class_ids, original_img_shape\n",
    "\n",
    "def draw_boxes_on_image(image_np, boxes, confidences, class_ids, fps_text=''):\n",
    "    image_np = image_np.copy()\n",
    "    names = settings.ALL_CLASSES\n",
    "    for i in range(len(boxes)):\n",
    "        x1, y1, x2, y2 = boxes[i]\n",
    "        confidence = confidences[i]\n",
    "        class_id = class_ids[i]\n",
    "        if class_id < len(names):\n",
    "            class_name = names[class_id]\n",
    "            color = (0, 255, 0)\n",
    "            text = f'{remove_dash_from_class_name(class_name)}: {confidence:.2f}'\n",
    "            cv2.rectangle(image_np, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(image_np, text, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    cv2.putText(image_np, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, False)\n",
    "    return image_np\n",
    "\n",
    "@app.get('/', response_class=HTMLResponse)\n",
    "async def serve_index():\n",
    "    with open('static/index.html', 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "@app.post('/detect')\n",
    "async def detect_image(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        contents = await file.read()\n",
    "        image = Image.open(io.BytesIO(contents)).convert('RGB')\n",
    "        image_np = np.array(image)\n",
    "        image_np = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
    "        original_shape = image_np.shape[:2]\n",
    "        input_image = preprocess_image_for_rknn(image_np)\n",
    "        outputs = rknn.inference(inputs=[input_image])\n",
    "        boxes, confidences, class_ids, _ = postprocess_yolov8_rknn_output(outputs, original_shape)\n",
    "        detected_items = [settings.ALL_CLASSES[cls_id] for cls_id in class_ids if cls_id < len(settings.ALL_CLASSES)]\n",
    "        recyclable_items, non_recyclable_items, hazardous_items = classify_waste_type(detected_items)\n",
    "        result_dict = {\n",
    "            'recyclable': [remove_dash_from_class_name(item) for item in recyclable_items],\n",
    "            'non_recyclable': [remove_dash_from_class_name(item) for item in non_recyclable_items],\n",
    "            'hazardous': [remove_dash_from_class_name(item) for item in hazardous_items]\n",
    "        }\n",
    "        plotted_image = draw_boxes_on_image(image_np, boxes, confidences, class_ids)\n",
    "        _, buffer = cv2.imencode('.jpg', plotted_image)\n",
    "        encoded_image = base64.b64encode(buffer).decode('utf-8')\n",
    "        return {'results': result_dict, 'image': encoded_image}\n",
    "    except Exception as e:\n",
    "        print(f'Error during image detection: {e}')\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get('/video_feed')\n",
    "async def video_feed():\n",
    "    global latest_webcam_results\n",
    "    webcam_stop_event.clear()\n",
    "    print('Backend: Webcam stream started, stop event cleared.')\n",
    "    async def generate():\n",
    "        cap = cv2.VideoCapture(settings.WEBCAM_PATH)\n",
    "        if not cap.isOpened():\n",
    "            print('Backend: Error: Could not open webcam.')\n",
    "            yield (b'--frame\\r\\n'\n",
    "                   b'Content-Type: image/jpeg\\r\\n\\r\\n' +\n",
    "                   cv2.imencode('.jpg', np.zeros((480, 640, 3), dtype=np.uint8))[1].tobytes() + b'\\r\\n')\n",
    "            return\n",
    "        print('Backend: Webcam opened successfully. Streaming frames...')\n",
    "        prev_frame_time = 0\n",
    "        new_frame_time = 0\n",
    "        try:\n",
    "            while True:\n",
    "                if webcam_stop_event.is_set():\n",
    "                    print('Backend: Webcam stop event detected. Breaking loop.')\n",
    "                    break\n",
    "                success, frame = cap.read()\n",
    "                if not success:\n",
    "                    print('Backend: Error: Failed to read frame from webcam. Breaking loop.')\n",
    "                    break\n",
    "                new_frame_time = time.time()\n",
    "                fps = 1 / (new_frame_time - prev_frame_time)\n",
    "                prev_frame_time = new_frame_time\n",
    "                fps_text = f'FPS: {int(fps)}'\n",
    "                original_shape = frame.shape[:2]\n",
    "                input_image = preprocess_image_for_rknn(frame)\n",
    "                outputs = rknn.inference(inputs=[input_image])\n",
    "                boxes, confidences, class_ids, _ = postprocess_yolov8_rknn_output(outputs, original_shape)\n",
    "                detected_items = [settings.ALL_CLASSES[cls_id] for cls_id in class_ids if cls_id < len(settings.ALL_CLASSES)]\n",
    "                recyclable_items, non_recyclable_items, hazardous_items = classify_waste_type(detected_items)\n",
    "                latest_webcam_results['recyclable'] = [remove_dash_from_class_name(item) for item in recyclable_items]\n",
    "                latest_webcam_results['non_recyclable'] = [remove_dash_from_class_name(item) for item in non_recyclable_items]\n",
    "                latest_webcam_results['hazardous'] = [remove_dash_from_class_name(item) for item in hazardous_items]\n",
    "                plotted_image = draw_boxes_on_image(frame, boxes, confidences, class_ids, fps_text)\n",
    "                _, buffer = cv2.imencode('.jpg', plotted_image)\n",
    "                frame_bytes = buffer.tobytes()\n",
    "                yield (b'--frame\\r\\n'\n",
    "                       b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame_bytes + b'\\r\\n')\n",
    "                await asyncio.sleep(0.001)\n",
    "        finally:\n",
    "            if cap.isOpened():\n",
    "                cap.release()\n",
    "                print('Backend: Webcam released.')\n",
    "            else:\n",
    "                print('Backend: Webcam was not opened, nothing to release.')\n",
    "    return StreamingResponse(generate(), media_type='multipart/x-mixed-replace;boundary=frame')\n",
    "\n",
    "@app.post('/stop_webcam_backend')\n",
    "async def stop_webcam_backend():\n",
    "    webcam_stop_event.set()\n",
    "    print('Backend: Received stop signal from frontend. Event set.')\n",
    "    return JSONResponse(content={'message': 'Webcam stop signal sent.'})\n",
    "\n",
    "@app.get('/webcam_classification')\n",
    "async def get_webcam_classification():\n",
    "    return JSONResponse(content=latest_webcam_results)\n",
    "\n",
    "@app.on_event('shutdown')\n",
    "def cleanup():\n",
    "    rknn.release()\n",
    "    print('Backend: RKNN model released.')\n",
    "\"\"\"\n",
    "\n",
    "with open(BASE_DIR / 'main1.py', 'w') as f:\n",
    "    f.write(main1_py_content)\n",
    "\n",
    "print('main1.py created successfully.')\n",
    "\n",
    "# Note: To run the RKNN FastAPI app, copy the following files to your OrangePi5Pro:\n",
    "# - weights/best-rk3588.rknn\n",
    "# - settings.py\n",
    "# - main1.py\n",
    "# - static/index.html\n",
    "# Then run: uvicorn main1:app --host 0.0.0.0 --port 8000\n",
    "# Ensure the RKNN toolkit and dependencies are installed on the OrangePi5Pro."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
